{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_hurst2.ipynb ",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMi98euKWCzLOXHTfmze+6P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/u-keigo/RRIproject1/blob/main/LSTM_hurst2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "off0ybXVLahB"
      },
      "source": [
        "# ハースト指数を予測する\n",
        "## （失敗作）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvrAI48HLZs_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7d45a39-4f50-40a5-9ec6-1877bc8f7950"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1mjucopLnUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83cd7c9d-217f-411b-91e2-eb1d40a1a2da"
      },
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import linecache\n",
        "\n",
        "\n",
        "def read_Gauss(name):\n",
        "  nums = []  # 整数を入れるリスト\n",
        "  with open(name, 'r', encoding='utf-8') as fin:  # ファイルを開く\n",
        "    for line in fin.readlines():  # 行を読み込んでfor文で回す\n",
        "        try:\n",
        "            line = line.replace('\\n','')\n",
        "            num = float(line)  # 行を整数（int）に変換する\n",
        "        except ValueError as e:\n",
        "            print(e, file=sys.stderr)  # エラーが出たら画面に出力\n",
        "            continue\n",
        "\n",
        "        nums.append(num)  # 変換した整数をリストに保存する\n",
        "  return (nums)\n",
        "\n",
        "\n",
        "# カテゴリを配列で取得\n",
        "drive_dir = \"/content/drive/My Drive/python/\"\n",
        "\n",
        "categories = [name for name in os.listdir(drive_dir + 'data_gauss') if os.path.isdir(drive_dir + \"data_gauss/\" +name)]\n",
        "print(categories)\n",
        "\n",
        "# datasets = pd.DataFrame(columns=[\"data\", \"hurst\"])\n",
        "# for cat in categories:\n",
        "#     path = drive_dir + \"data_gauss/\" + cat + \"/*.rri\"\n",
        "#     files = glob(path)\n",
        "#     for text_name in files:\n",
        "#       data = pd.DataFrame(read_Gauss(text_name)).T  # 転置\n",
        "#       s = pd.Series([data, cat], index=datasets.columns)\n",
        "#       datasets = datasets.append(s, ignore_index=True)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['H=0.1', 'H=0.9', 'H=0.7', 'H=0.8', 'H=0.6', 'H=0.5', 'H=0.4', 'H=0.3', 'H=0.2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "AD5YAHdlctKv",
        "outputId": "4a0c9d28-1d86-442c-a44a-0464c696bc52"
      },
      "source": [
        "import numpy as np\n",
        "DAT = pd.DataFrame(np.zeros(shape=(1024, (32*9))))\n",
        "i=0\n",
        "for cat in categories:\n",
        "    path = drive_dir + \"data_gauss/\" + cat + \"/*.rri\"\n",
        "    files = glob(path)\n",
        "    for text_name in files:\n",
        "      data = pd.Series(read_Gauss(text_name))\n",
        "      DAT.iloc[:, i] = data\n",
        "      DAT.rename(columns={i: cat}, inplace=True)\n",
        "      i = i+1\n",
        "# print(DAT)\n",
        "\n",
        "# データフレームシャッフル\n",
        "# datasets = datasets.sample(frac=1).reset_index(drop=True)\n",
        "# datasets.head()\n",
        "\n",
        "# データフレームシャッフル\n",
        "DAT = DAT.sample(frac=1,axis=1).reset_index(drop=True)\n",
        "# DAT.head()\n",
        "DAT.T.head()\n",
        "\n",
        "# DAT = DAT.T"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>984</th>\n",
              "      <th>985</th>\n",
              "      <th>986</th>\n",
              "      <th>987</th>\n",
              "      <th>988</th>\n",
              "      <th>989</th>\n",
              "      <th>990</th>\n",
              "      <th>991</th>\n",
              "      <th>992</th>\n",
              "      <th>993</th>\n",
              "      <th>994</th>\n",
              "      <th>995</th>\n",
              "      <th>996</th>\n",
              "      <th>997</th>\n",
              "      <th>998</th>\n",
              "      <th>999</th>\n",
              "      <th>1000</th>\n",
              "      <th>1001</th>\n",
              "      <th>1002</th>\n",
              "      <th>1003</th>\n",
              "      <th>1004</th>\n",
              "      <th>1005</th>\n",
              "      <th>1006</th>\n",
              "      <th>1007</th>\n",
              "      <th>1008</th>\n",
              "      <th>1009</th>\n",
              "      <th>1010</th>\n",
              "      <th>1011</th>\n",
              "      <th>1012</th>\n",
              "      <th>1013</th>\n",
              "      <th>1014</th>\n",
              "      <th>1015</th>\n",
              "      <th>1016</th>\n",
              "      <th>1017</th>\n",
              "      <th>1018</th>\n",
              "      <th>1019</th>\n",
              "      <th>1020</th>\n",
              "      <th>1021</th>\n",
              "      <th>1022</th>\n",
              "      <th>1023</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>H=0.2</th>\n",
              "      <td>-1.130373</td>\n",
              "      <td>-0.428530</td>\n",
              "      <td>-0.393066</td>\n",
              "      <td>-0.471474</td>\n",
              "      <td>0.568630</td>\n",
              "      <td>0.557980</td>\n",
              "      <td>-1.171732</td>\n",
              "      <td>0.467845</td>\n",
              "      <td>1.798626</td>\n",
              "      <td>-1.215588</td>\n",
              "      <td>0.322395</td>\n",
              "      <td>-0.494266</td>\n",
              "      <td>-0.338830</td>\n",
              "      <td>1.326324</td>\n",
              "      <td>-1.199087</td>\n",
              "      <td>0.213739</td>\n",
              "      <td>0.373788</td>\n",
              "      <td>0.368113</td>\n",
              "      <td>-0.773283</td>\n",
              "      <td>-1.726757</td>\n",
              "      <td>1.604321</td>\n",
              "      <td>0.794513</td>\n",
              "      <td>0.424914</td>\n",
              "      <td>-0.302916</td>\n",
              "      <td>-0.743518</td>\n",
              "      <td>-0.265037</td>\n",
              "      <td>0.390143</td>\n",
              "      <td>0.538971</td>\n",
              "      <td>-1.583546</td>\n",
              "      <td>0.576373</td>\n",
              "      <td>2.503756</td>\n",
              "      <td>-0.618127</td>\n",
              "      <td>1.300475</td>\n",
              "      <td>-1.142879</td>\n",
              "      <td>0.068889</td>\n",
              "      <td>-0.000835</td>\n",
              "      <td>-0.656284</td>\n",
              "      <td>0.337572</td>\n",
              "      <td>0.102868</td>\n",
              "      <td>0.051326</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.582739</td>\n",
              "      <td>-0.925454</td>\n",
              "      <td>0.793244</td>\n",
              "      <td>-0.075495</td>\n",
              "      <td>0.925343</td>\n",
              "      <td>1.577879</td>\n",
              "      <td>-0.254060</td>\n",
              "      <td>-0.543843</td>\n",
              "      <td>-0.303507</td>\n",
              "      <td>0.956111</td>\n",
              "      <td>-0.734804</td>\n",
              "      <td>1.099500</td>\n",
              "      <td>-0.060795</td>\n",
              "      <td>-0.068063</td>\n",
              "      <td>0.859841</td>\n",
              "      <td>-1.618380</td>\n",
              "      <td>0.525018</td>\n",
              "      <td>-0.445469</td>\n",
              "      <td>1.251462</td>\n",
              "      <td>0.789095</td>\n",
              "      <td>0.265766</td>\n",
              "      <td>-0.580667</td>\n",
              "      <td>-0.261300</td>\n",
              "      <td>-1.764644</td>\n",
              "      <td>2.066552</td>\n",
              "      <td>-0.189467</td>\n",
              "      <td>0.491882</td>\n",
              "      <td>0.081474</td>\n",
              "      <td>-0.556621</td>\n",
              "      <td>0.355490</td>\n",
              "      <td>-0.274303</td>\n",
              "      <td>-0.802754</td>\n",
              "      <td>-0.083473</td>\n",
              "      <td>0.185315</td>\n",
              "      <td>0.990196</td>\n",
              "      <td>1.181386</td>\n",
              "      <td>-0.205434</td>\n",
              "      <td>0.195855</td>\n",
              "      <td>0.386719</td>\n",
              "      <td>-0.197017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H=0.3</th>\n",
              "      <td>-0.270630</td>\n",
              "      <td>-0.052219</td>\n",
              "      <td>0.882561</td>\n",
              "      <td>0.390187</td>\n",
              "      <td>-0.765130</td>\n",
              "      <td>0.357391</td>\n",
              "      <td>-1.629637</td>\n",
              "      <td>0.371252</td>\n",
              "      <td>-0.021151</td>\n",
              "      <td>1.401296</td>\n",
              "      <td>0.713391</td>\n",
              "      <td>-0.156416</td>\n",
              "      <td>-0.472903</td>\n",
              "      <td>-0.584165</td>\n",
              "      <td>-0.254039</td>\n",
              "      <td>2.636086</td>\n",
              "      <td>0.518914</td>\n",
              "      <td>-0.541038</td>\n",
              "      <td>-1.104555</td>\n",
              "      <td>0.844501</td>\n",
              "      <td>-1.685569</td>\n",
              "      <td>-1.529202</td>\n",
              "      <td>0.734706</td>\n",
              "      <td>1.562057</td>\n",
              "      <td>0.796333</td>\n",
              "      <td>0.350613</td>\n",
              "      <td>-2.417091</td>\n",
              "      <td>0.766400</td>\n",
              "      <td>-0.382830</td>\n",
              "      <td>1.257232</td>\n",
              "      <td>-0.874011</td>\n",
              "      <td>2.424291</td>\n",
              "      <td>-0.419121</td>\n",
              "      <td>1.492447</td>\n",
              "      <td>-1.065733</td>\n",
              "      <td>0.282410</td>\n",
              "      <td>-0.005991</td>\n",
              "      <td>-0.469199</td>\n",
              "      <td>1.053221</td>\n",
              "      <td>-0.468093</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.016265</td>\n",
              "      <td>0.482592</td>\n",
              "      <td>-0.433166</td>\n",
              "      <td>0.052452</td>\n",
              "      <td>-0.814353</td>\n",
              "      <td>-0.071982</td>\n",
              "      <td>0.833211</td>\n",
              "      <td>-0.685864</td>\n",
              "      <td>-1.182330</td>\n",
              "      <td>1.069065</td>\n",
              "      <td>-0.473949</td>\n",
              "      <td>-0.055178</td>\n",
              "      <td>-0.183784</td>\n",
              "      <td>0.292320</td>\n",
              "      <td>-0.510515</td>\n",
              "      <td>0.606760</td>\n",
              "      <td>-1.050334</td>\n",
              "      <td>0.017618</td>\n",
              "      <td>-0.034730</td>\n",
              "      <td>0.304188</td>\n",
              "      <td>-1.085935</td>\n",
              "      <td>0.152691</td>\n",
              "      <td>-0.296577</td>\n",
              "      <td>-0.611090</td>\n",
              "      <td>0.843364</td>\n",
              "      <td>0.928616</td>\n",
              "      <td>0.602687</td>\n",
              "      <td>0.020884</td>\n",
              "      <td>1.318332</td>\n",
              "      <td>1.756914</td>\n",
              "      <td>-0.795196</td>\n",
              "      <td>-0.504210</td>\n",
              "      <td>0.031195</td>\n",
              "      <td>-0.424033</td>\n",
              "      <td>0.857375</td>\n",
              "      <td>-2.212068</td>\n",
              "      <td>0.962941</td>\n",
              "      <td>0.710978</td>\n",
              "      <td>-0.593960</td>\n",
              "      <td>0.453911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H=0.2</th>\n",
              "      <td>0.845461</td>\n",
              "      <td>-2.134457</td>\n",
              "      <td>0.603385</td>\n",
              "      <td>-0.495729</td>\n",
              "      <td>-0.704056</td>\n",
              "      <td>0.918272</td>\n",
              "      <td>0.517226</td>\n",
              "      <td>-1.776804</td>\n",
              "      <td>0.545644</td>\n",
              "      <td>-0.060963</td>\n",
              "      <td>1.278737</td>\n",
              "      <td>-1.287791</td>\n",
              "      <td>0.537247</td>\n",
              "      <td>0.304055</td>\n",
              "      <td>-0.146142</td>\n",
              "      <td>0.447847</td>\n",
              "      <td>0.363029</td>\n",
              "      <td>0.660639</td>\n",
              "      <td>-1.270708</td>\n",
              "      <td>0.371136</td>\n",
              "      <td>-0.258741</td>\n",
              "      <td>0.359309</td>\n",
              "      <td>-0.546128</td>\n",
              "      <td>0.655015</td>\n",
              "      <td>-0.376205</td>\n",
              "      <td>-0.697187</td>\n",
              "      <td>0.086764</td>\n",
              "      <td>0.252157</td>\n",
              "      <td>0.388421</td>\n",
              "      <td>-1.334445</td>\n",
              "      <td>0.987701</td>\n",
              "      <td>0.020077</td>\n",
              "      <td>0.837301</td>\n",
              "      <td>-1.128812</td>\n",
              "      <td>0.734057</td>\n",
              "      <td>-1.249393</td>\n",
              "      <td>0.741410</td>\n",
              "      <td>-1.014634</td>\n",
              "      <td>-0.561648</td>\n",
              "      <td>-0.569003</td>\n",
              "      <td>...</td>\n",
              "      <td>0.687570</td>\n",
              "      <td>-0.947287</td>\n",
              "      <td>1.216429</td>\n",
              "      <td>-0.581162</td>\n",
              "      <td>0.865358</td>\n",
              "      <td>0.915445</td>\n",
              "      <td>-0.775494</td>\n",
              "      <td>-0.248142</td>\n",
              "      <td>0.115697</td>\n",
              "      <td>0.874609</td>\n",
              "      <td>-2.019860</td>\n",
              "      <td>1.240208</td>\n",
              "      <td>-1.079387</td>\n",
              "      <td>0.454000</td>\n",
              "      <td>1.633866</td>\n",
              "      <td>0.374936</td>\n",
              "      <td>-1.351849</td>\n",
              "      <td>-0.233934</td>\n",
              "      <td>2.166926</td>\n",
              "      <td>-1.195369</td>\n",
              "      <td>-0.663495</td>\n",
              "      <td>-0.131335</td>\n",
              "      <td>-0.254910</td>\n",
              "      <td>-0.360757</td>\n",
              "      <td>1.710536</td>\n",
              "      <td>-0.971714</td>\n",
              "      <td>-0.179198</td>\n",
              "      <td>1.626823</td>\n",
              "      <td>-0.360994</td>\n",
              "      <td>-0.288578</td>\n",
              "      <td>0.391594</td>\n",
              "      <td>-0.522012</td>\n",
              "      <td>-0.132008</td>\n",
              "      <td>-0.263979</td>\n",
              "      <td>2.017746</td>\n",
              "      <td>-0.202105</td>\n",
              "      <td>-1.697152</td>\n",
              "      <td>0.688652</td>\n",
              "      <td>-1.242811</td>\n",
              "      <td>0.387248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H=0.3</th>\n",
              "      <td>0.270713</td>\n",
              "      <td>0.257660</td>\n",
              "      <td>-1.302111</td>\n",
              "      <td>0.653043</td>\n",
              "      <td>0.791902</td>\n",
              "      <td>1.443553</td>\n",
              "      <td>-1.882534</td>\n",
              "      <td>0.007123</td>\n",
              "      <td>1.876918</td>\n",
              "      <td>-0.249543</td>\n",
              "      <td>-0.599939</td>\n",
              "      <td>0.084230</td>\n",
              "      <td>0.104813</td>\n",
              "      <td>0.402340</td>\n",
              "      <td>-0.991412</td>\n",
              "      <td>0.902232</td>\n",
              "      <td>1.342981</td>\n",
              "      <td>1.158316</td>\n",
              "      <td>-0.318776</td>\n",
              "      <td>0.327386</td>\n",
              "      <td>-0.320492</td>\n",
              "      <td>0.201141</td>\n",
              "      <td>-1.263361</td>\n",
              "      <td>0.967571</td>\n",
              "      <td>-1.299746</td>\n",
              "      <td>-0.347711</td>\n",
              "      <td>-0.467678</td>\n",
              "      <td>0.103069</td>\n",
              "      <td>0.880427</td>\n",
              "      <td>0.583121</td>\n",
              "      <td>-1.466506</td>\n",
              "      <td>-0.434635</td>\n",
              "      <td>-0.031756</td>\n",
              "      <td>0.206658</td>\n",
              "      <td>0.129278</td>\n",
              "      <td>0.168461</td>\n",
              "      <td>0.077070</td>\n",
              "      <td>0.164448</td>\n",
              "      <td>0.534932</td>\n",
              "      <td>1.083266</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.238616</td>\n",
              "      <td>-1.041325</td>\n",
              "      <td>1.322185</td>\n",
              "      <td>0.188579</td>\n",
              "      <td>-0.311717</td>\n",
              "      <td>1.581445</td>\n",
              "      <td>-0.899375</td>\n",
              "      <td>1.625957</td>\n",
              "      <td>-0.603206</td>\n",
              "      <td>0.328240</td>\n",
              "      <td>0.751340</td>\n",
              "      <td>-1.247165</td>\n",
              "      <td>0.681159</td>\n",
              "      <td>0.363328</td>\n",
              "      <td>0.256396</td>\n",
              "      <td>0.667333</td>\n",
              "      <td>-1.094254</td>\n",
              "      <td>-0.716918</td>\n",
              "      <td>-1.321456</td>\n",
              "      <td>1.147096</td>\n",
              "      <td>0.529748</td>\n",
              "      <td>1.310854</td>\n",
              "      <td>1.080830</td>\n",
              "      <td>-0.408979</td>\n",
              "      <td>0.433511</td>\n",
              "      <td>-0.883981</td>\n",
              "      <td>0.474659</td>\n",
              "      <td>-0.876102</td>\n",
              "      <td>-0.792861</td>\n",
              "      <td>-0.768437</td>\n",
              "      <td>0.239419</td>\n",
              "      <td>0.448190</td>\n",
              "      <td>2.064420</td>\n",
              "      <td>-1.572814</td>\n",
              "      <td>0.653447</td>\n",
              "      <td>0.912916</td>\n",
              "      <td>0.188359</td>\n",
              "      <td>0.454284</td>\n",
              "      <td>0.634286</td>\n",
              "      <td>0.540301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H=0.6</th>\n",
              "      <td>1.062003</td>\n",
              "      <td>0.043896</td>\n",
              "      <td>0.271069</td>\n",
              "      <td>0.614364</td>\n",
              "      <td>1.308715</td>\n",
              "      <td>1.698036</td>\n",
              "      <td>0.273378</td>\n",
              "      <td>-0.749291</td>\n",
              "      <td>-0.761834</td>\n",
              "      <td>-1.313716</td>\n",
              "      <td>-0.130227</td>\n",
              "      <td>0.390479</td>\n",
              "      <td>0.009100</td>\n",
              "      <td>0.819734</td>\n",
              "      <td>0.607889</td>\n",
              "      <td>-0.941508</td>\n",
              "      <td>0.693636</td>\n",
              "      <td>-0.979254</td>\n",
              "      <td>2.321902</td>\n",
              "      <td>-0.149643</td>\n",
              "      <td>0.186116</td>\n",
              "      <td>0.746830</td>\n",
              "      <td>-0.210770</td>\n",
              "      <td>-0.029781</td>\n",
              "      <td>-0.642259</td>\n",
              "      <td>-1.628246</td>\n",
              "      <td>0.813899</td>\n",
              "      <td>-1.775983</td>\n",
              "      <td>1.627653</td>\n",
              "      <td>0.080419</td>\n",
              "      <td>0.611430</td>\n",
              "      <td>0.872858</td>\n",
              "      <td>0.684777</td>\n",
              "      <td>1.534029</td>\n",
              "      <td>1.613324</td>\n",
              "      <td>-0.282668</td>\n",
              "      <td>0.233070</td>\n",
              "      <td>0.689317</td>\n",
              "      <td>-0.516095</td>\n",
              "      <td>-0.020499</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.154555</td>\n",
              "      <td>0.141850</td>\n",
              "      <td>-0.717408</td>\n",
              "      <td>1.101262</td>\n",
              "      <td>-0.523269</td>\n",
              "      <td>-0.895608</td>\n",
              "      <td>1.069527</td>\n",
              "      <td>0.171681</td>\n",
              "      <td>-2.013537</td>\n",
              "      <td>-0.996152</td>\n",
              "      <td>-2.471893</td>\n",
              "      <td>-0.087673</td>\n",
              "      <td>0.410652</td>\n",
              "      <td>0.677489</td>\n",
              "      <td>0.295762</td>\n",
              "      <td>0.209844</td>\n",
              "      <td>-0.625745</td>\n",
              "      <td>1.281559</td>\n",
              "      <td>-1.622033</td>\n",
              "      <td>0.652187</td>\n",
              "      <td>0.923096</td>\n",
              "      <td>0.281349</td>\n",
              "      <td>-0.272761</td>\n",
              "      <td>-0.496268</td>\n",
              "      <td>-1.076766</td>\n",
              "      <td>-1.361444</td>\n",
              "      <td>-0.057868</td>\n",
              "      <td>0.301805</td>\n",
              "      <td>-0.312192</td>\n",
              "      <td>0.465718</td>\n",
              "      <td>-0.350188</td>\n",
              "      <td>-2.315996</td>\n",
              "      <td>-1.828743</td>\n",
              "      <td>0.135778</td>\n",
              "      <td>0.713835</td>\n",
              "      <td>-0.212863</td>\n",
              "      <td>1.369095</td>\n",
              "      <td>-1.090713</td>\n",
              "      <td>0.156519</td>\n",
              "      <td>-0.371390</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1024 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2     ...      1021      1022      1023\n",
              "H=0.2 -1.130373 -0.428530 -0.393066  ...  0.195855  0.386719 -0.197017\n",
              "H=0.3 -0.270630 -0.052219  0.882561  ...  0.710978 -0.593960  0.453911\n",
              "H=0.2  0.845461 -2.134457  0.603385  ...  0.688652 -1.242811  0.387248\n",
              "H=0.3  0.270713  0.257660 -1.302111  ...  0.454284  0.634286  0.540301\n",
              "H=0.6  1.062003  0.043896  0.271069  ... -1.090713  0.156519 -0.371390\n",
              "\n",
              "[5 rows x 1024 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nQbIVvqdDWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd374dd5-d1b3-47da-96ac-c303ff937db0"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "category2index = {}\n",
        "for cat in categories:\n",
        "    if cat in category2index: continue\n",
        "    category2index[cat] = len(category2index)\n",
        "print(category2index)\n",
        "#{'movie-enter': 0, 'it-life-hack': 1, 'kaden-channel': 2, 'topic-news': 3, 'livedoor-homme': 4, 'peachy': 5, 'sports-watch': 6, 'dokujo-tsushin': 7, 'smax': 8}\n",
        "\n",
        "def category2tensor(cat):\n",
        "    return torch.tensor([category2index[cat]], dtype=torch.long)\n",
        "\n",
        "# データフレームの形状変更・indexの要素化\n",
        "# df = DAT.T\n",
        "df = DAT.T.drop(['H=0.2','H=0.3','H=0.4','H=0.6','H=0.7','H=0.8'])\n",
        "df['index'] = df.index\n",
        "df.head()\n",
        "\n",
        "dlen = len(DAT.columns)  # 288\n",
        "train_data, test_data = train_test_split(df, test_size=0.3,random_state=0)\n",
        "# train_data_np = np.asarray(train_data)\n",
        "# train_label = train_data.values.tolist()\n",
        "# test_data_np = np.asarray(test_data)\n",
        "# test_label = test_data.index.values.tolist()\n",
        "# データの形状を確認\n",
        "print(\"train_data size: {}\". format(train_data.shape))\n",
        "print(\"test_data size: {}\". format(test_data.shape))\n",
        "# print(\"train_label size: {}\". format(train_label.shape))\n",
        "# print(\"test_label size: {}\". format(test_label.shape))\n",
        "# print(train_label)\n",
        "\n",
        "# ndarrayをPytorchのTensorに変換\n",
        "# train_x = torch.Tensor(train_data_np)\n",
        "# test_x = torch.Tensor(test_data_np)\n",
        "# train_y = category2tensor(train_label)\n",
        "# test_y = category2tensor(test_label)\n",
        "# print(\"train_data size: {}\". format(train_x.shape))\n",
        "# print(\"test_data size: {}\". format(test_x.shape))\n",
        "\n",
        "# 特徴量とラベルを結合したデータセットを作成\n",
        "# train_dataset = TensorDataset(train_x, train_y)\n",
        "# test_dataset = TensorDataset(test_x, test_y)\n",
        "\n",
        "\n",
        "# DataLoaderを使って、データセットを128個のミニパッチに分ける\n",
        "# ミニパッチサイズを指定したデータローダを作成\n",
        "train_batch = DataLoader(\n",
        "    dataset = train_data,   # データセットの指定\n",
        "    batch_size = 128,   # バッチサイズの指定\n",
        "    shuffle = True,    # シャッフルするかどうかの指定\n",
        "    num_workers = 2)   # コアの数\n",
        "\n",
        "test_batch = DataLoader(\n",
        "    dataset = test_data,   # データセットの指定\n",
        "    batch_size = 128,   # バッチサイズの指定\n",
        "    shuffle = False,    # シャッフルするかどうかの指定\n",
        "    num_workers = 2)   # コアの数\n",
        "\n",
        "# ミニバッチデータセットの確認\n",
        "# for data, label in train_batch:\n",
        "#   print(\"batch data size: {}\". format(data.size()))  # バッチの入力データサイズ\n",
        "#   print(\"batch label size: {}\". format(label.size()))   # バッチのラベルサイズ\n",
        "#   break\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'H=0.1': 0, 'H=0.9': 1, 'H=0.7': 2, 'H=0.8': 3, 'H=0.6': 4, 'H=0.5': 5, 'H=0.4': 6, 'H=0.3': 7, 'H=0.2': 8}\n",
            "train_data size: (67, 1025)\n",
            "test_data size: (29, 1025)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPSXmCWA6pc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89fdfdc4-a63e-4d03-9eac-e1a74b693e13"
      },
      "source": [
        "# nn.Moduleを継承して新しいクラスを作る。決まり文句\n",
        "class LSTMClassifier(nn.Module):\n",
        "    # モデルで使う各ネットワークをコンストラクタで定義\n",
        "    def __init__(self, embedding_dim, hidden_dim, tagset_size):\n",
        "        # 親クラスのコンストラクタ。決まり文句\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # インプットの単語をベクトル化するために使う\n",
        "        # self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # LSTMの隠れ層。これ１つでOK。超便利。\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "        # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    # 順伝播処理はforward関数に記載\n",
        "    def forward(self, sentence):\n",
        "        # 文章内の各単語をベクトル化して出力。2次元のテンソル\n",
        "        # embeds = self.word_embeddings(sentence)\n",
        "        embeds = sentence\n",
        "        # 2次元テンソルをLSTMに食わせられる様にviewで３次元テンソルにした上でLSTMへ流す。\n",
        "        # 上記で説明した様にmany to oneのタスクを解きたいので、第二戻り値だけ使う。\n",
        "        _, lstm_out = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        # lstm_out[0]は３次元テンソルになってしまっているので2次元に調整して全結合。\n",
        "        tag_space = self.hidden2tag(lstm_out[0].view(-1, self.hidden_dim))\n",
        "        # softmaxに食わせて、確率として表現\n",
        "        tag_scores = self.softmax(tag_space)\n",
        "        return tag_scores\n",
        "\n",
        "category2index = {}\n",
        "for cat in categories:\n",
        "    if cat in category2index: continue\n",
        "    category2index[cat] = len(category2index)\n",
        "print(category2index)\n",
        "#{'movie-enter': 0, 'it-life-hack': 1, 'kaden-channel': 2, 'topic-news': 3, 'livedoor-homme': 4, 'peachy': 5, 'sports-watch': 6, 'dokujo-tsushin': 7, 'smax': 8}\n",
        "\n",
        "def category2tensor(cat):\n",
        "    return torch.tensor([category2index[cat]], dtype=torch.long)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'H=0.1': 0, 'H=0.9': 1, 'H=0.7': 2, 'H=0.8': 3, 'H=0.6': 4, 'H=0.5': 5, 'H=0.4': 6, 'H=0.3': 7, 'H=0.2': 8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmcMN4RA9XjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42ddaf53-51f0-450c-8c94-521e0e413c02"
      },
      "source": [
        "# print(train_data.index)\n",
        "# 入力次元数\n",
        "EMBEDDING_DIM = 1\n",
        "# EMBEDDING_DIM = 10\n",
        "# 隠れ層の次元数\n",
        "HIDDEN_DIM = 128\n",
        "# データ全体の単語数\n",
        "# VOCAB_SIZE = len(word2index)\n",
        "# 分類先のカテゴリの数\n",
        "TAG_SIZE = len(categories)\n",
        "\n",
        "\n",
        "# ネットワークのロード\n",
        "# CPUとGPUのどちらを使うかを指定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "net = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, TAG_SIZE).to(device)\n",
        "# デバイスの確認\n",
        "print(\"Device: {}\".format(device))\n",
        "\n",
        "\n",
        "# モデル宣言\n",
        "# model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, TAG_SIZE)\n",
        "# 損失関数はNLLLoss()を使う。LogSoftmaxを使う時はこれを使うらしい。\n",
        "loss_function = nn.NLLLoss()\n",
        "# 最適化関数の定義\n",
        "# optimizer = optim.Adam(net.parameters())\n",
        "# 最適化の手法はSGDで。lossの減りに時間かかるけど、一旦はこれを使う。\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 損失を保存するリストを作成\n",
        "train_loss_list = []  # 学習損失\n",
        "test_loss_list = []  # 評価損失\n",
        "\n",
        "epoch = 100\n",
        "\n",
        "# 各エポックの合計loss値を格納する\n",
        "losses = []\n",
        "# 100ループ回してみる。（バッチ化とかGPU使ってないので結構時間かかる...）\n",
        "for i in range(epoch):\n",
        "  all_loss = 0\n",
        "  # エポックの進行状況を表示\n",
        "  print('-------------------------------------')\n",
        "  print(\"Epoch: {}/{}\".format(i+1,epoch))\n",
        "\n",
        "  # 損失の初期化\n",
        "  # train_loss = 0  # 学習損失\n",
        "  # test_loss = 0  # 評価損失\n",
        "\n",
        "  # ---学習パート--- #\n",
        "  # ニューラルネットワークを学習モードに設定\n",
        "  # net.train()\n",
        "  for ii in range(0,len(train_data)):\n",
        "        data = train_data.iloc[ii][:-1]\n",
        "        # cat = train_data.iloc[ii][0]\n",
        "        cat = train_data.iloc[ii]['index']\n",
        "        # print(data)\n",
        "        # print(cat)\n",
        "        # モデルが持ってる勾配の情報をリセット\n",
        "        # model.zero_grad()\n",
        "        net.zero_grad()\n",
        "        # 文章を単語IDの系列に変換（modelに食わせられる形に変換）\n",
        "        # inputs = torch.Tensor(data.values.astype(np.float32))\n",
        "        inputs = torch.tensor(np.array(data.astype('f')))\n",
        "        # GPUにTensorを転送\n",
        "        inputs = inputs.to(device)\n",
        "        # 順伝播の結果を受け取る\n",
        "        # out = model(inputs)\n",
        "        out = net(inputs)\n",
        "        # 正解カテゴリをテンソル化\n",
        "        answer = category2tensor(cat)\n",
        "        # GPUにTensorを転送\n",
        "        answer = answer.to(device)\n",
        "        # 正解とのlossを計算\n",
        "        loss = loss_function(out, answer)\n",
        "        # 勾配をセット\n",
        "        loss.backward()\n",
        "        # 逆伝播でパラメータ更新\n",
        "        optimizer.step()\n",
        "        # lossを集計\n",
        "        all_loss += loss.item()\n",
        "  losses.append(all_loss)\n",
        "  print(\"epoch\", i+1, \"\\t\" , \"loss\", all_loss)\n",
        "print(\"done.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "-------------------------------------\n",
            "Epoch: 1/100\n",
            "epoch 1 \t loss 142.15049767494202\n",
            "-------------------------------------\n",
            "Epoch: 2/100\n",
            "epoch 2 \t loss 130.09253644943237\n",
            "-------------------------------------\n",
            "Epoch: 3/100\n",
            "epoch 3 \t loss 119.63391768932343\n",
            "-------------------------------------\n",
            "Epoch: 4/100\n",
            "epoch 4 \t loss 110.41628873348236\n",
            "-------------------------------------\n",
            "Epoch: 5/100\n",
            "epoch 5 \t loss 102.3537745475769\n",
            "-------------------------------------\n",
            "Epoch: 6/100\n",
            "epoch 6 \t loss 95.52632009983063\n",
            "-------------------------------------\n",
            "Epoch: 7/100\n",
            "epoch 7 \t loss 90.03826701641083\n",
            "-------------------------------------\n",
            "Epoch: 8/100\n",
            "epoch 8 \t loss 85.88086140155792\n",
            "-------------------------------------\n",
            "Epoch: 9/100\n",
            "epoch 9 \t loss 82.88964223861694\n",
            "-------------------------------------\n",
            "Epoch: 10/100\n",
            "epoch 10 \t loss 80.80723863840103\n",
            "-------------------------------------\n",
            "Epoch: 11/100\n",
            "epoch 11 \t loss 79.37394452095032\n",
            "-------------------------------------\n",
            "Epoch: 12/100\n",
            "epoch 12 \t loss 78.38134729862213\n",
            "-------------------------------------\n",
            "Epoch: 13/100\n",
            "epoch 13 \t loss 77.68231117725372\n",
            "-------------------------------------\n",
            "Epoch: 14/100\n",
            "epoch 14 \t loss 77.17923766374588\n",
            "-------------------------------------\n",
            "Epoch: 15/100\n",
            "epoch 15 \t loss 76.8087494969368\n",
            "-------------------------------------\n",
            "Epoch: 16/100\n",
            "epoch 16 \t loss 76.5296066403389\n",
            "-------------------------------------\n",
            "Epoch: 17/100\n",
            "epoch 17 \t loss 76.31464511156082\n",
            "-------------------------------------\n",
            "Epoch: 18/100\n",
            "epoch 18 \t loss 76.14566540718079\n",
            "-------------------------------------\n",
            "Epoch: 19/100\n",
            "epoch 19 \t loss 76.01024496555328\n",
            "-------------------------------------\n",
            "Epoch: 20/100\n",
            "epoch 20 \t loss 75.89973723888397\n",
            "-------------------------------------\n",
            "Epoch: 21/100\n",
            "epoch 21 \t loss 75.80802780389786\n",
            "-------------------------------------\n",
            "Epoch: 22/100\n",
            "epoch 22 \t loss 75.73070627450943\n",
            "-------------------------------------\n",
            "Epoch: 23/100\n",
            "epoch 23 \t loss 75.66454291343689\n",
            "-------------------------------------\n",
            "Epoch: 24/100\n",
            "epoch 24 \t loss 75.60714226961136\n",
            "-------------------------------------\n",
            "Epoch: 25/100\n",
            "epoch 25 \t loss 75.55669683218002\n",
            "-------------------------------------\n",
            "Epoch: 26/100\n",
            "epoch 26 \t loss 75.5118311047554\n",
            "-------------------------------------\n",
            "Epoch: 27/100\n",
            "epoch 27 \t loss 75.47148418426514\n",
            "-------------------------------------\n",
            "Epoch: 28/100\n",
            "epoch 28 \t loss 75.4348219037056\n",
            "-------------------------------------\n",
            "Epoch: 29/100\n",
            "epoch 29 \t loss 75.40119570493698\n",
            "-------------------------------------\n",
            "Epoch: 30/100\n",
            "epoch 30 \t loss 75.37008512020111\n",
            "-------------------------------------\n",
            "Epoch: 31/100\n",
            "epoch 31 \t loss 75.34107428789139\n",
            "-------------------------------------\n",
            "Epoch: 32/100\n",
            "epoch 32 \t loss 75.31383144855499\n",
            "-------------------------------------\n",
            "Epoch: 33/100\n",
            "epoch 33 \t loss 75.28807997703552\n",
            "-------------------------------------\n",
            "Epoch: 34/100\n",
            "epoch 34 \t loss 75.26359540224075\n",
            "-------------------------------------\n",
            "Epoch: 35/100\n",
            "epoch 35 \t loss 75.2401933670044\n",
            "-------------------------------------\n",
            "Epoch: 36/100\n",
            "epoch 36 \t loss 75.21772515773773\n",
            "-------------------------------------\n",
            "Epoch: 37/100\n",
            "epoch 37 \t loss 75.19606101512909\n",
            "-------------------------------------\n",
            "Epoch: 38/100\n",
            "epoch 38 \t loss 75.1750927567482\n",
            "-------------------------------------\n",
            "Epoch: 39/100\n",
            "epoch 39 \t loss 75.15473240613937\n",
            "-------------------------------------\n",
            "Epoch: 40/100\n",
            "epoch 40 \t loss 75.13490009307861\n",
            "-------------------------------------\n",
            "Epoch: 41/100\n",
            "epoch 41 \t loss 75.1155321598053\n",
            "-------------------------------------\n",
            "Epoch: 42/100\n",
            "epoch 42 \t loss 75.09657150506973\n",
            "-------------------------------------\n",
            "Epoch: 43/100\n",
            "epoch 43 \t loss 75.07797008752823\n",
            "-------------------------------------\n",
            "Epoch: 44/100\n",
            "epoch 44 \t loss 75.05968606472015\n",
            "-------------------------------------\n",
            "Epoch: 45/100\n",
            "epoch 45 \t loss 75.04168105125427\n",
            "-------------------------------------\n",
            "Epoch: 46/100\n",
            "epoch 46 \t loss 75.02392613887787\n",
            "-------------------------------------\n",
            "Epoch: 47/100\n",
            "epoch 47 \t loss 75.00638860464096\n",
            "-------------------------------------\n",
            "Epoch: 48/100\n",
            "epoch 48 \t loss 74.98904466629028\n",
            "-------------------------------------\n",
            "Epoch: 49/100\n",
            "epoch 49 \t loss 74.97187459468842\n",
            "-------------------------------------\n",
            "Epoch: 50/100\n",
            "epoch 50 \t loss 74.9548442363739\n",
            "-------------------------------------\n",
            "Epoch: 51/100\n",
            "epoch 51 \t loss 74.93794637918472\n",
            "-------------------------------------\n",
            "Epoch: 52/100\n",
            "epoch 52 \t loss 74.9211620092392\n",
            "-------------------------------------\n",
            "Epoch: 53/100\n",
            "epoch 53 \t loss 74.90446430444717\n",
            "-------------------------------------\n",
            "Epoch: 54/100\n",
            "epoch 54 \t loss 74.88784682750702\n",
            "-------------------------------------\n",
            "Epoch: 55/100\n",
            "epoch 55 \t loss 74.87128686904907\n",
            "-------------------------------------\n",
            "Epoch: 56/100\n",
            "epoch 56 \t loss 74.8547796010971\n",
            "-------------------------------------\n",
            "Epoch: 57/100\n",
            "epoch 57 \t loss 74.83829522132874\n",
            "-------------------------------------\n",
            "Epoch: 58/100\n",
            "epoch 58 \t loss 74.82182770967484\n",
            "-------------------------------------\n",
            "Epoch: 59/100\n",
            "epoch 59 \t loss 74.80536150932312\n",
            "-------------------------------------\n",
            "Epoch: 60/100\n",
            "epoch 60 \t loss 74.78887712955475\n",
            "-------------------------------------\n",
            "Epoch: 61/100\n",
            "epoch 61 \t loss 74.7723633646965\n",
            "-------------------------------------\n",
            "Epoch: 62/100\n",
            "epoch 62 \t loss 74.755799472332\n",
            "-------------------------------------\n",
            "Epoch: 63/100\n",
            "epoch 63 \t loss 74.7391704916954\n",
            "-------------------------------------\n",
            "Epoch: 64/100\n",
            "epoch 64 \t loss 74.72245252132416\n",
            "-------------------------------------\n",
            "Epoch: 65/100\n",
            "epoch 65 \t loss 74.70563733577728\n",
            "-------------------------------------\n",
            "Epoch: 66/100\n",
            "epoch 66 \t loss 74.68869316577911\n",
            "-------------------------------------\n",
            "Epoch: 67/100\n",
            "epoch 67 \t loss 74.67160528898239\n",
            "-------------------------------------\n",
            "Epoch: 68/100\n",
            "epoch 68 \t loss 74.65434753894806\n",
            "-------------------------------------\n",
            "Epoch: 69/100\n",
            "epoch 69 \t loss 74.63689267635345\n",
            "-------------------------------------\n",
            "Epoch: 70/100\n",
            "epoch 70 \t loss 74.61921715736389\n",
            "-------------------------------------\n",
            "Epoch: 71/100\n",
            "epoch 71 \t loss 74.60128808021545\n",
            "-------------------------------------\n",
            "Epoch: 72/100\n",
            "epoch 72 \t loss 74.58307015895844\n",
            "-------------------------------------\n",
            "Epoch: 73/100\n",
            "epoch 73 \t loss 74.56453168392181\n",
            "-------------------------------------\n",
            "Epoch: 74/100\n",
            "epoch 74 \t loss 74.54562968015671\n",
            "-------------------------------------\n",
            "Epoch: 75/100\n",
            "epoch 75 \t loss 74.52631562948227\n",
            "-------------------------------------\n",
            "Epoch: 76/100\n",
            "epoch 76 \t loss 74.50654584169388\n",
            "-------------------------------------\n",
            "Epoch: 77/100\n",
            "epoch 77 \t loss 74.48626691102982\n",
            "-------------------------------------\n",
            "Epoch: 78/100\n",
            "epoch 78 \t loss 74.46540576219559\n",
            "-------------------------------------\n",
            "Epoch: 79/100\n",
            "epoch 79 \t loss 74.44390559196472\n",
            "-------------------------------------\n",
            "Epoch: 80/100\n",
            "epoch 80 \t loss 74.42168354988098\n",
            "-------------------------------------\n",
            "Epoch: 81/100\n",
            "epoch 81 \t loss 74.39865148067474\n",
            "-------------------------------------\n",
            "Epoch: 82/100\n",
            "epoch 82 \t loss 74.37470829486847\n",
            "-------------------------------------\n",
            "Epoch: 83/100\n",
            "epoch 83 \t loss 74.34973567724228\n",
            "-------------------------------------\n",
            "Epoch: 84/100\n",
            "epoch 84 \t loss 74.32360625267029\n",
            "-------------------------------------\n",
            "Epoch: 85/100\n",
            "epoch 85 \t loss 74.29616624116898\n",
            "-------------------------------------\n",
            "Epoch: 86/100\n",
            "epoch 86 \t loss 74.2672364115715\n",
            "-------------------------------------\n",
            "Epoch: 87/100\n",
            "epoch 87 \t loss 74.23661011457443\n",
            "-------------------------------------\n",
            "Epoch: 88/100\n",
            "epoch 88 \t loss 74.20405209064484\n",
            "-------------------------------------\n",
            "Epoch: 89/100\n",
            "epoch 89 \t loss 74.16927254199982\n",
            "-------------------------------------\n",
            "Epoch: 90/100\n",
            "epoch 90 \t loss 74.13193410634995\n",
            "-------------------------------------\n",
            "Epoch: 91/100\n",
            "epoch 91 \t loss 74.09162670373917\n",
            "-------------------------------------\n",
            "Epoch: 92/100\n",
            "epoch 92 \t loss 74.04787307977676\n",
            "-------------------------------------\n",
            "Epoch: 93/100\n",
            "epoch 93 \t loss 74.00006473064423\n",
            "-------------------------------------\n",
            "Epoch: 94/100\n",
            "epoch 94 \t loss 73.94748049974442\n",
            "-------------------------------------\n",
            "Epoch: 95/100\n",
            "epoch 95 \t loss 73.88921922445297\n",
            "-------------------------------------\n",
            "Epoch: 96/100\n",
            "epoch 96 \t loss 73.82415914535522\n",
            "-------------------------------------\n",
            "Epoch: 97/100\n",
            "epoch 97 \t loss 73.7508761882782\n",
            "-------------------------------------\n",
            "Epoch: 98/100\n",
            "epoch 98 \t loss 73.66757744550705\n",
            "-------------------------------------\n",
            "Epoch: 99/100\n",
            "epoch 99 \t loss 73.57197910547256\n",
            "-------------------------------------\n",
            "Epoch: 100/100\n",
            "epoch 100 \t loss 73.46122652292252\n",
            "done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "W37NkMH49FFa",
        "outputId": "cb92f8e1-8c58-40dc-e9bc-6e20d340c520"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# 損失\n",
        "fig2 = plt.figure()\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(range(1, epoch+1), losses)\n",
        "plt.show()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "O6NGzlPnA347",
        "outputId": "e265686c-ab68-4e09-93ad-b54c9e9b47dd"
      },
      "source": [
        "traindata_num = len(train_data)\n",
        "a = 0\n",
        "with torch.no_grad():\n",
        "    for ii in range(0,len(train_data)):\n",
        "        data = train_data.iloc[ii][:-1]\n",
        "        # cat = train_data.iloc[ii][0]\n",
        "        cat = train_data.iloc[ii]['index']\n",
        "\n",
        "        inputs = torch.tensor(np.array(data.astype('f')))\n",
        "        # GPUにTensorを転送\n",
        "        inputs = inputs.to(device)\n",
        "        # 順伝播の結果を受け取る\n",
        "        out = net(inputs)\n",
        "        # 正解カテゴリをテンソル化\n",
        "        answer = category2tensor(cat)\n",
        "        # GPUにTensorを転送\n",
        "        answer = answer.to(device)\n",
        "        # 正解とのlossを計算\n",
        "        # loss = loss_function(out, answer)\n",
        "        # inputs = sentence2index(title)\n",
        "        # out = model(inputs)\n",
        "        _, predict = torch.max(out, 1)\n",
        "        answer = category2tensor(category)\n",
        "        if predict == answer:\n",
        "            a += 1\n",
        "print(\"predict : \", a / traindata_num)\n",
        "# predict :  0.9984505132674801\n",
        "\n",
        "import collections\n",
        "# IDをカテゴリに戻す用\n",
        "index2category = {}\n",
        "for cat, idx in category2index.items():\n",
        "    index2category[idx] = cat\n",
        "\n",
        "# answer -> 正解ラベル、predict->LSTMの予測結果、exact->正解してたらO,間違っていたらX\n",
        "predict_df = pd.DataFrame(columns=[\"answer\", \"predict\", \"exact\"])\n",
        "\n",
        "# 予測して結果を上のDFに格納\n",
        "with torch.no_grad():\n",
        "    for ii in range(0,len(test_data)):\n",
        "        data = test_data.iloc[ii][:-1]\n",
        "        cat = test_data.iloc[ii]['index']\n",
        "\n",
        "        inputs = torch.tensor(np.array(data.astype('f')))\n",
        "        # GPUにTensorを転送\n",
        "        inputs = inputs.to(device)\n",
        "        # 順伝播の結果を受け取る\n",
        "        out = net(inputs)\n",
        "        # 正解カテゴリをテンソル化\n",
        "        # answer = category2tensor(cat)\n",
        "        # GPUにTensorを転送\n",
        "        # answer = answer.to(device)\n",
        "\n",
        "        # out = model(sentence2index(title))\n",
        "        _, predict = torch.max(out, 1)\n",
        "        answer = category2tensor(category)\n",
        "        exact = \"O\" if predict.item() == answer.item() else \"X\"\n",
        "        s = pd.Series([answer.item(), predict.item(), exact], index=predict_df.columns)\n",
        "        predict_df = predict_df.append(s, ignore_index=True)\n",
        "\n",
        "# Fスコア格納用のDF\n",
        "fscore_df = pd.DataFrame(columns=[\"category\", \"all\",\"precison\", \"recall\", \"fscore\"])\n",
        "\n",
        "# 分類器が答えた各カテゴリの件数\n",
        "prediction_count = collections.Counter(predict_df[\"predict\"])\n",
        "# 各カテゴリの総件数\n",
        "answer_count = collections.Counter(predict_df[\"answer\"])\n",
        "\n",
        "# Fスコア求める\n",
        "for i in range(9):\n",
        "    all_count = answer_count[i]\n",
        "    precision = len(predict_df.query('predict == ' + str(i) + ' and exact == \"O\"')) / prediction_count[i]\n",
        "    recall = len(predict_df.query('answer == ' + str(i) + ' and exact == \"O\"')) / all_count\n",
        "    fscore = 2*precision*recall / (precision + recall)\n",
        "    s = pd.Series([index2category[i], all_count, round(precision, 2), round(recall, 2), round(fscore, 2)], index=fscore_df.columns)\n",
        "    fscore_df = fscore_df.append(s, ignore_index=True)\n",
        "print(fscore_df)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ec0388838aca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# out = model(inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategory2tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpredict\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'category' is not defined"
          ]
        }
      ]
    }
  ]
}
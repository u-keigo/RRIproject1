{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_hurst2.ipynb ",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyMIVgrX/lwwuAVjfbRVuudQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/u-keigo/RRIproject1/blob/main/LSTM_hurst2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "off0ybXVLahB"
      },
      "source": [
        "# ハースト指数を予測する\n",
        "## （失敗作）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvrAI48HLZs_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "567c2cef-7d6c-435f-850b-dc790dfdfbee"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1mjucopLnUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aa5271f-295b-45e9-950f-1fcd91647a5c"
      },
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import linecache\n",
        "\n",
        "\n",
        "def read_Gauss(name):\n",
        "  nums = []  # 整数を入れるリスト\n",
        "  with open(name, 'r', encoding='utf-8') as fin:  # ファイルを開く\n",
        "    for line in fin.readlines():  # 行を読み込んでfor文で回す\n",
        "        try:\n",
        "            line = line.replace('\\n','')\n",
        "            num = float(line)  # 行を整数（int）に変換する\n",
        "        except ValueError as e:\n",
        "            print(e, file=sys.stderr)  # エラーが出たら画面に出力\n",
        "            continue\n",
        "\n",
        "        nums.append(num)  # 変換した整数をリストに保存する\n",
        "  return (nums)\n",
        "\n",
        "\n",
        "# カテゴリを配列で取得\n",
        "drive_dir = \"/content/drive/My Drive/python/\"\n",
        "\n",
        "categories = [name for name in os.listdir(drive_dir + 'data_gauss') if os.path.isdir(drive_dir + \"data_gauss/\" +name)]\n",
        "print(categories)\n",
        "\n",
        "# datasets = pd.DataFrame(columns=[\"data\", \"hurst\"])\n",
        "# for cat in categories:\n",
        "#     path = drive_dir + \"data_gauss/\" + cat + \"/*.rri\"\n",
        "#     files = glob(path)\n",
        "#     for text_name in files:\n",
        "#       data = pd.DataFrame(read_Gauss(text_name)).T  # 転置\n",
        "#       s = pd.Series([data, cat], index=datasets.columns)\n",
        "#       datasets = datasets.append(s, ignore_index=True)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['H=0.1', 'H=0.9', 'H=0.7', 'H=0.8', 'H=0.6', 'H=0.5', 'H=0.4', 'H=0.3', 'H=0.2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "AD5YAHdlctKv",
        "outputId": "11d55883-c629-4d24-f430-7d913d04ae01"
      },
      "source": [
        "import numpy as np\n",
        "DAT = pd.DataFrame(np.zeros(shape=(1024, (32*9))))\n",
        "i=0\n",
        "for cat in categories:\n",
        "    path = drive_dir + \"data_gauss/\" + cat + \"/*.rri\"\n",
        "    files = glob(path)\n",
        "    for text_name in files:\n",
        "      data = pd.Series(read_Gauss(text_name))\n",
        "      DAT.iloc[:, i] = data\n",
        "      DAT.rename(columns={i: cat}, inplace=True)\n",
        "      i = i+1\n",
        "# print(DAT)\n",
        "\n",
        "# データフレームシャッフル\n",
        "# datasets = datasets.sample(frac=1).reset_index(drop=True)\n",
        "# datasets.head()\n",
        "\n",
        "# データフレームシャッフル\n",
        "DAT = DAT.sample(frac=1,axis=1).reset_index(drop=True)\n",
        "# DAT.head()\n",
        "DAT.T.head()\n",
        "\n",
        "# DAT = DAT.T"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>984</th>\n",
              "      <th>985</th>\n",
              "      <th>986</th>\n",
              "      <th>987</th>\n",
              "      <th>988</th>\n",
              "      <th>989</th>\n",
              "      <th>990</th>\n",
              "      <th>991</th>\n",
              "      <th>992</th>\n",
              "      <th>993</th>\n",
              "      <th>994</th>\n",
              "      <th>995</th>\n",
              "      <th>996</th>\n",
              "      <th>997</th>\n",
              "      <th>998</th>\n",
              "      <th>999</th>\n",
              "      <th>1000</th>\n",
              "      <th>1001</th>\n",
              "      <th>1002</th>\n",
              "      <th>1003</th>\n",
              "      <th>1004</th>\n",
              "      <th>1005</th>\n",
              "      <th>1006</th>\n",
              "      <th>1007</th>\n",
              "      <th>1008</th>\n",
              "      <th>1009</th>\n",
              "      <th>1010</th>\n",
              "      <th>1011</th>\n",
              "      <th>1012</th>\n",
              "      <th>1013</th>\n",
              "      <th>1014</th>\n",
              "      <th>1015</th>\n",
              "      <th>1016</th>\n",
              "      <th>1017</th>\n",
              "      <th>1018</th>\n",
              "      <th>1019</th>\n",
              "      <th>1020</th>\n",
              "      <th>1021</th>\n",
              "      <th>1022</th>\n",
              "      <th>1023</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>H=0.8</th>\n",
              "      <td>1.081220</td>\n",
              "      <td>-0.466223</td>\n",
              "      <td>0.448821</td>\n",
              "      <td>-0.270061</td>\n",
              "      <td>0.955563</td>\n",
              "      <td>-0.132322</td>\n",
              "      <td>1.026051</td>\n",
              "      <td>0.428904</td>\n",
              "      <td>-0.877347</td>\n",
              "      <td>-0.252242</td>\n",
              "      <td>1.141460</td>\n",
              "      <td>1.012073</td>\n",
              "      <td>0.270662</td>\n",
              "      <td>0.672422</td>\n",
              "      <td>-0.614371</td>\n",
              "      <td>0.316036</td>\n",
              "      <td>0.990671</td>\n",
              "      <td>-0.220835</td>\n",
              "      <td>-0.123766</td>\n",
              "      <td>-0.288017</td>\n",
              "      <td>0.599795</td>\n",
              "      <td>0.413915</td>\n",
              "      <td>1.438428</td>\n",
              "      <td>1.694147</td>\n",
              "      <td>1.570087</td>\n",
              "      <td>2.009134</td>\n",
              "      <td>1.707526</td>\n",
              "      <td>1.797674</td>\n",
              "      <td>0.802184</td>\n",
              "      <td>0.898621</td>\n",
              "      <td>0.665646</td>\n",
              "      <td>-0.950622</td>\n",
              "      <td>0.065790</td>\n",
              "      <td>-0.657510</td>\n",
              "      <td>-0.448334</td>\n",
              "      <td>0.379653</td>\n",
              "      <td>0.682335</td>\n",
              "      <td>0.322825</td>\n",
              "      <td>0.530928</td>\n",
              "      <td>-0.493479</td>\n",
              "      <td>...</td>\n",
              "      <td>0.473240</td>\n",
              "      <td>0.169879</td>\n",
              "      <td>-0.050818</td>\n",
              "      <td>0.264690</td>\n",
              "      <td>0.829137</td>\n",
              "      <td>-0.254991</td>\n",
              "      <td>-0.366313</td>\n",
              "      <td>1.003994</td>\n",
              "      <td>0.857811</td>\n",
              "      <td>-1.373315</td>\n",
              "      <td>0.208462</td>\n",
              "      <td>-0.723135</td>\n",
              "      <td>0.260181</td>\n",
              "      <td>-1.313718</td>\n",
              "      <td>0.283033</td>\n",
              "      <td>-0.700299</td>\n",
              "      <td>-0.945350</td>\n",
              "      <td>-0.480835</td>\n",
              "      <td>0.248203</td>\n",
              "      <td>0.021129</td>\n",
              "      <td>-1.026322</td>\n",
              "      <td>0.259089</td>\n",
              "      <td>-0.818444</td>\n",
              "      <td>-0.723609</td>\n",
              "      <td>-1.530818</td>\n",
              "      <td>-1.444361</td>\n",
              "      <td>-1.356688</td>\n",
              "      <td>-0.868234</td>\n",
              "      <td>-0.760057</td>\n",
              "      <td>0.378079</td>\n",
              "      <td>-0.590334</td>\n",
              "      <td>-0.880650</td>\n",
              "      <td>0.918100</td>\n",
              "      <td>-0.995229</td>\n",
              "      <td>0.180448</td>\n",
              "      <td>-0.276451</td>\n",
              "      <td>-0.126465</td>\n",
              "      <td>-1.100336</td>\n",
              "      <td>-0.796695</td>\n",
              "      <td>-0.366983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H=0.2</th>\n",
              "      <td>2.349821</td>\n",
              "      <td>-1.098292</td>\n",
              "      <td>0.206474</td>\n",
              "      <td>-0.699417</td>\n",
              "      <td>-0.979692</td>\n",
              "      <td>1.185743</td>\n",
              "      <td>-1.353455</td>\n",
              "      <td>0.197714</td>\n",
              "      <td>-0.455217</td>\n",
              "      <td>0.819474</td>\n",
              "      <td>0.853858</td>\n",
              "      <td>-1.134331</td>\n",
              "      <td>0.189728</td>\n",
              "      <td>0.670844</td>\n",
              "      <td>0.360995</td>\n",
              "      <td>0.223842</td>\n",
              "      <td>0.295898</td>\n",
              "      <td>-0.467135</td>\n",
              "      <td>0.305479</td>\n",
              "      <td>0.533374</td>\n",
              "      <td>-0.182241</td>\n",
              "      <td>-0.761533</td>\n",
              "      <td>0.496710</td>\n",
              "      <td>-1.205217</td>\n",
              "      <td>-0.414505</td>\n",
              "      <td>0.732610</td>\n",
              "      <td>0.906869</td>\n",
              "      <td>-0.969266</td>\n",
              "      <td>-0.545976</td>\n",
              "      <td>1.172338</td>\n",
              "      <td>-0.810165</td>\n",
              "      <td>-0.477207</td>\n",
              "      <td>0.379367</td>\n",
              "      <td>2.142363</td>\n",
              "      <td>-1.030570</td>\n",
              "      <td>-0.138550</td>\n",
              "      <td>-0.837162</td>\n",
              "      <td>0.783635</td>\n",
              "      <td>-1.677740</td>\n",
              "      <td>1.578123</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.979011</td>\n",
              "      <td>1.047802</td>\n",
              "      <td>-0.396257</td>\n",
              "      <td>0.625490</td>\n",
              "      <td>-1.216494</td>\n",
              "      <td>0.025856</td>\n",
              "      <td>1.122956</td>\n",
              "      <td>-0.580375</td>\n",
              "      <td>-1.829485</td>\n",
              "      <td>0.445813</td>\n",
              "      <td>2.360200</td>\n",
              "      <td>-2.184127</td>\n",
              "      <td>2.371423</td>\n",
              "      <td>0.063979</td>\n",
              "      <td>0.243639</td>\n",
              "      <td>0.037973</td>\n",
              "      <td>0.257532</td>\n",
              "      <td>0.932046</td>\n",
              "      <td>-0.191536</td>\n",
              "      <td>1.107596</td>\n",
              "      <td>-0.836854</td>\n",
              "      <td>-0.734388</td>\n",
              "      <td>0.841930</td>\n",
              "      <td>-2.728639</td>\n",
              "      <td>1.394709</td>\n",
              "      <td>-0.500120</td>\n",
              "      <td>0.349409</td>\n",
              "      <td>0.170111</td>\n",
              "      <td>0.339199</td>\n",
              "      <td>-0.738828</td>\n",
              "      <td>-0.093201</td>\n",
              "      <td>-0.129033</td>\n",
              "      <td>-0.363034</td>\n",
              "      <td>0.336361</td>\n",
              "      <td>-1.547395</td>\n",
              "      <td>1.749766</td>\n",
              "      <td>0.407874</td>\n",
              "      <td>-0.493725</td>\n",
              "      <td>0.550379</td>\n",
              "      <td>0.506152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H=0.9</th>\n",
              "      <td>1.318264</td>\n",
              "      <td>0.853099</td>\n",
              "      <td>1.176719</td>\n",
              "      <td>-0.096603</td>\n",
              "      <td>0.032307</td>\n",
              "      <td>-0.004347</td>\n",
              "      <td>-0.306752</td>\n",
              "      <td>-0.301542</td>\n",
              "      <td>0.557565</td>\n",
              "      <td>-0.118395</td>\n",
              "      <td>-0.433116</td>\n",
              "      <td>-0.269064</td>\n",
              "      <td>-1.417738</td>\n",
              "      <td>-1.288668</td>\n",
              "      <td>-1.249530</td>\n",
              "      <td>-0.285020</td>\n",
              "      <td>-0.041512</td>\n",
              "      <td>-0.304697</td>\n",
              "      <td>-0.500136</td>\n",
              "      <td>0.175904</td>\n",
              "      <td>0.455460</td>\n",
              "      <td>0.376668</td>\n",
              "      <td>0.403881</td>\n",
              "      <td>1.372776</td>\n",
              "      <td>1.032165</td>\n",
              "      <td>-0.038328</td>\n",
              "      <td>-0.087098</td>\n",
              "      <td>0.522925</td>\n",
              "      <td>0.209257</td>\n",
              "      <td>0.553176</td>\n",
              "      <td>-0.312711</td>\n",
              "      <td>-0.243420</td>\n",
              "      <td>0.063152</td>\n",
              "      <td>-0.472047</td>\n",
              "      <td>-0.382859</td>\n",
              "      <td>-0.408662</td>\n",
              "      <td>-0.604408</td>\n",
              "      <td>-0.612729</td>\n",
              "      <td>0.217037</td>\n",
              "      <td>-1.016204</td>\n",
              "      <td>...</td>\n",
              "      <td>1.088469</td>\n",
              "      <td>1.211031</td>\n",
              "      <td>1.649463</td>\n",
              "      <td>1.366937</td>\n",
              "      <td>1.215562</td>\n",
              "      <td>0.521412</td>\n",
              "      <td>0.729830</td>\n",
              "      <td>0.363814</td>\n",
              "      <td>0.149953</td>\n",
              "      <td>-0.297791</td>\n",
              "      <td>0.512051</td>\n",
              "      <td>0.241238</td>\n",
              "      <td>0.280507</td>\n",
              "      <td>-0.378437</td>\n",
              "      <td>0.097900</td>\n",
              "      <td>0.448632</td>\n",
              "      <td>-0.176819</td>\n",
              "      <td>0.181371</td>\n",
              "      <td>-0.368593</td>\n",
              "      <td>0.551827</td>\n",
              "      <td>0.336692</td>\n",
              "      <td>0.299602</td>\n",
              "      <td>-0.167670</td>\n",
              "      <td>0.712639</td>\n",
              "      <td>0.120785</td>\n",
              "      <td>-0.328219</td>\n",
              "      <td>-0.063728</td>\n",
              "      <td>-0.186684</td>\n",
              "      <td>0.563453</td>\n",
              "      <td>0.436234</td>\n",
              "      <td>1.133757</td>\n",
              "      <td>0.860582</td>\n",
              "      <td>1.233934</td>\n",
              "      <td>0.210437</td>\n",
              "      <td>-0.423220</td>\n",
              "      <td>-0.839056</td>\n",
              "      <td>-0.195591</td>\n",
              "      <td>-0.476278</td>\n",
              "      <td>-0.534649</td>\n",
              "      <td>0.942192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H=0.9</th>\n",
              "      <td>-1.377474</td>\n",
              "      <td>-0.524521</td>\n",
              "      <td>-1.390828</td>\n",
              "      <td>-2.331807</td>\n",
              "      <td>-1.824477</td>\n",
              "      <td>-1.395063</td>\n",
              "      <td>-1.539889</td>\n",
              "      <td>-1.743832</td>\n",
              "      <td>-2.067304</td>\n",
              "      <td>-1.234445</td>\n",
              "      <td>0.037610</td>\n",
              "      <td>-0.233607</td>\n",
              "      <td>-1.463204</td>\n",
              "      <td>-0.842157</td>\n",
              "      <td>-1.376860</td>\n",
              "      <td>-0.827061</td>\n",
              "      <td>-1.290072</td>\n",
              "      <td>-1.149924</td>\n",
              "      <td>-1.106254</td>\n",
              "      <td>-1.448530</td>\n",
              "      <td>-1.793017</td>\n",
              "      <td>-0.512032</td>\n",
              "      <td>-1.100220</td>\n",
              "      <td>-1.797649</td>\n",
              "      <td>-0.772432</td>\n",
              "      <td>-1.062906</td>\n",
              "      <td>-0.210246</td>\n",
              "      <td>-1.349159</td>\n",
              "      <td>-0.494662</td>\n",
              "      <td>-1.663152</td>\n",
              "      <td>-0.726361</td>\n",
              "      <td>-1.186176</td>\n",
              "      <td>0.903329</td>\n",
              "      <td>0.235523</td>\n",
              "      <td>-0.388771</td>\n",
              "      <td>-0.611115</td>\n",
              "      <td>-0.628022</td>\n",
              "      <td>-0.790578</td>\n",
              "      <td>-0.983201</td>\n",
              "      <td>-1.619559</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.438244</td>\n",
              "      <td>-1.365152</td>\n",
              "      <td>-0.788250</td>\n",
              "      <td>-0.084149</td>\n",
              "      <td>0.211608</td>\n",
              "      <td>-1.951193</td>\n",
              "      <td>-1.502755</td>\n",
              "      <td>-1.064303</td>\n",
              "      <td>-1.621049</td>\n",
              "      <td>-1.432032</td>\n",
              "      <td>-2.590128</td>\n",
              "      <td>-2.378314</td>\n",
              "      <td>-2.405253</td>\n",
              "      <td>-1.691840</td>\n",
              "      <td>-1.804896</td>\n",
              "      <td>-1.168007</td>\n",
              "      <td>-1.165539</td>\n",
              "      <td>-1.415047</td>\n",
              "      <td>-0.154842</td>\n",
              "      <td>-0.649902</td>\n",
              "      <td>-1.360119</td>\n",
              "      <td>-0.147231</td>\n",
              "      <td>-1.608744</td>\n",
              "      <td>-1.906187</td>\n",
              "      <td>-0.054851</td>\n",
              "      <td>-0.571294</td>\n",
              "      <td>-0.720339</td>\n",
              "      <td>-1.316556</td>\n",
              "      <td>-1.119316</td>\n",
              "      <td>-0.340377</td>\n",
              "      <td>-0.624661</td>\n",
              "      <td>-0.962480</td>\n",
              "      <td>-0.321520</td>\n",
              "      <td>-1.302981</td>\n",
              "      <td>0.782560</td>\n",
              "      <td>-0.336910</td>\n",
              "      <td>-1.822888</td>\n",
              "      <td>-0.504277</td>\n",
              "      <td>-0.483810</td>\n",
              "      <td>-0.767977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H=0.4</th>\n",
              "      <td>-0.780532</td>\n",
              "      <td>0.115989</td>\n",
              "      <td>0.208394</td>\n",
              "      <td>0.385630</td>\n",
              "      <td>1.422088</td>\n",
              "      <td>-0.341390</td>\n",
              "      <td>-0.483914</td>\n",
              "      <td>0.114750</td>\n",
              "      <td>1.045467</td>\n",
              "      <td>-1.242310</td>\n",
              "      <td>-0.943463</td>\n",
              "      <td>1.891369</td>\n",
              "      <td>-0.675751</td>\n",
              "      <td>2.255156</td>\n",
              "      <td>-0.203973</td>\n",
              "      <td>-0.486299</td>\n",
              "      <td>-0.470503</td>\n",
              "      <td>0.297299</td>\n",
              "      <td>0.402749</td>\n",
              "      <td>1.972319</td>\n",
              "      <td>-0.072595</td>\n",
              "      <td>-0.822167</td>\n",
              "      <td>-0.965147</td>\n",
              "      <td>0.999454</td>\n",
              "      <td>-0.739039</td>\n",
              "      <td>-0.741960</td>\n",
              "      <td>0.282054</td>\n",
              "      <td>-1.561822</td>\n",
              "      <td>-0.631089</td>\n",
              "      <td>1.435976</td>\n",
              "      <td>-0.067428</td>\n",
              "      <td>0.666723</td>\n",
              "      <td>-0.226097</td>\n",
              "      <td>-0.390372</td>\n",
              "      <td>0.071125</td>\n",
              "      <td>1.181393</td>\n",
              "      <td>1.685137</td>\n",
              "      <td>0.231603</td>\n",
              "      <td>-0.882087</td>\n",
              "      <td>-0.533814</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009644</td>\n",
              "      <td>0.220131</td>\n",
              "      <td>0.990605</td>\n",
              "      <td>0.195405</td>\n",
              "      <td>2.460764</td>\n",
              "      <td>-0.245678</td>\n",
              "      <td>-0.737297</td>\n",
              "      <td>1.096164</td>\n",
              "      <td>-1.255314</td>\n",
              "      <td>0.464159</td>\n",
              "      <td>-0.651257</td>\n",
              "      <td>-0.486440</td>\n",
              "      <td>-1.750244</td>\n",
              "      <td>-1.010852</td>\n",
              "      <td>0.217587</td>\n",
              "      <td>0.950562</td>\n",
              "      <td>-0.062550</td>\n",
              "      <td>0.010344</td>\n",
              "      <td>0.596334</td>\n",
              "      <td>0.526791</td>\n",
              "      <td>-0.051222</td>\n",
              "      <td>0.583735</td>\n",
              "      <td>-0.721764</td>\n",
              "      <td>-0.354554</td>\n",
              "      <td>0.339977</td>\n",
              "      <td>-0.061168</td>\n",
              "      <td>-0.379713</td>\n",
              "      <td>-1.080555</td>\n",
              "      <td>1.633215</td>\n",
              "      <td>0.628246</td>\n",
              "      <td>0.122047</td>\n",
              "      <td>-0.785767</td>\n",
              "      <td>-0.163620</td>\n",
              "      <td>-0.471456</td>\n",
              "      <td>-1.022953</td>\n",
              "      <td>-0.969309</td>\n",
              "      <td>-0.344384</td>\n",
              "      <td>0.228925</td>\n",
              "      <td>1.870254</td>\n",
              "      <td>0.380349</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1024 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2     ...      1021      1022      1023\n",
              "H=0.8  1.081220 -0.466223  0.448821  ... -1.100336 -0.796695 -0.366983\n",
              "H=0.2  2.349821 -1.098292  0.206474  ... -0.493725  0.550379  0.506152\n",
              "H=0.9  1.318264  0.853099  1.176719  ... -0.476278 -0.534649  0.942192\n",
              "H=0.9 -1.377474 -0.524521 -1.390828  ... -0.504277 -0.483810 -0.767977\n",
              "H=0.4 -0.780532  0.115989  0.208394  ...  0.228925  1.870254  0.380349\n",
              "\n",
              "[5 rows x 1024 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nQbIVvqdDWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c9a246-4fdb-49d3-81a6-d1709b426754"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "category2index = {}\n",
        "for cat in categories:\n",
        "    if cat in category2index: continue\n",
        "    category2index[cat] = len(category2index)\n",
        "print(category2index)\n",
        "#{'movie-enter': 0, 'it-life-hack': 1, 'kaden-channel': 2, 'topic-news': 3, 'livedoor-homme': 4, 'peachy': 5, 'sports-watch': 6, 'dokujo-tsushin': 7, 'smax': 8}\n",
        "\n",
        "def category2tensor(cat):\n",
        "    return torch.tensor([category2index[cat]], dtype=torch.long)\n",
        "\n",
        "# データフレームの形状変更・indexの要素化\n",
        "df = DAT.T\n",
        "df['index'] = df.index\n",
        "df.head()\n",
        "\n",
        "dlen = len(DAT.columns)  # 288\n",
        "train_data, test_data = train_test_split(df, test_size=0.3,random_state=0)\n",
        "# train_data_np = np.asarray(train_data)\n",
        "# train_label = train_data.values.tolist()\n",
        "# test_data_np = np.asarray(test_data)\n",
        "# test_label = test_data.index.values.tolist()\n",
        "# データの形状を確認\n",
        "print(\"train_data size: {}\". format(train_data.shape))\n",
        "print(\"test_data size: {}\". format(test_data.shape))\n",
        "# print(\"train_label size: {}\". format(train_label.shape))\n",
        "# print(\"test_label size: {}\". format(test_label.shape))\n",
        "# print(train_label)\n",
        "\n",
        "# ndarrayをPytorchのTensorに変換\n",
        "# train_x = torch.Tensor(train_data_np)\n",
        "# test_x = torch.Tensor(test_data_np)\n",
        "# train_y = category2tensor(train_label)\n",
        "# test_y = category2tensor(test_label)\n",
        "# print(\"train_data size: {}\". format(train_x.shape))\n",
        "# print(\"test_data size: {}\". format(test_x.shape))\n",
        "\n",
        "# 特徴量とラベルを結合したデータセットを作成\n",
        "# train_dataset = TensorDataset(train_x, train_y)\n",
        "# test_dataset = TensorDataset(test_x, test_y)\n",
        "\n",
        "\n",
        "# DataLoaderを使って、データセットを128個のミニパッチに分ける\n",
        "# ミニパッチサイズを指定したデータローダを作成\n",
        "train_batch = DataLoader(\n",
        "    dataset = train_data,   # データセットの指定\n",
        "    batch_size = 128,   # バッチサイズの指定\n",
        "    shuffle = True,    # シャッフルするかどうかの指定\n",
        "    num_workers = 2)   # コアの数\n",
        "\n",
        "test_batch = DataLoader(\n",
        "    dataset = test_data,   # データセットの指定\n",
        "    batch_size = 128,   # バッチサイズの指定\n",
        "    shuffle = False,    # シャッフルするかどうかの指定\n",
        "    num_workers = 2)   # コアの数\n",
        "\n",
        "# ミニバッチデータセットの確認\n",
        "# for data, label in train_batch:\n",
        "#   print(\"batch data size: {}\". format(data.size()))  # バッチの入力データサイズ\n",
        "#   print(\"batch label size: {}\". format(label.size()))   # バッチのラベルサイズ\n",
        "#   break\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'H=0.1': 0, 'H=0.9': 1, 'H=0.7': 2, 'H=0.8': 3, 'H=0.6': 4, 'H=0.5': 5, 'H=0.4': 6, 'H=0.3': 7, 'H=0.2': 8}\n",
            "train_data size: (201, 1025)\n",
            "test_data size: (87, 1025)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPSXmCWA6pc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42376e96-47d6-4b5c-e7bd-86c98bfa782d"
      },
      "source": [
        "# nn.Moduleを継承して新しいクラスを作る。決まり文句\n",
        "class LSTMClassifier(nn.Module):\n",
        "    # モデルで使う各ネットワークをコンストラクタで定義\n",
        "    def __init__(self, embedding_dim, hidden_dim, tagset_size):\n",
        "        # 親クラスのコンストラクタ。決まり文句\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # インプットの単語をベクトル化するために使う\n",
        "        # self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # LSTMの隠れ層。これ１つでOK。超便利。\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "        # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    # 順伝播処理はforward関数に記載\n",
        "    def forward(self, sentence):\n",
        "        # 文章内の各単語をベクトル化して出力。2次元のテンソル\n",
        "        # embeds = self.word_embeddings(sentence)\n",
        "        embeds = sentence\n",
        "        # 2次元テンソルをLSTMに食わせられる様にviewで３次元テンソルにした上でLSTMへ流す。\n",
        "        # 上記で説明した様にmany to oneのタスクを解きたいので、第二戻り値だけ使う。\n",
        "        _, lstm_out = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        # lstm_out[0]は３次元テンソルになってしまっているので2次元に調整して全結合。\n",
        "        tag_space = self.hidden2tag(lstm_out[0].view(-1, self.hidden_dim))\n",
        "        # softmaxに食わせて、確率として表現\n",
        "        tag_scores = self.softmax(tag_space)\n",
        "        return tag_scores\n",
        "\n",
        "category2index = {}\n",
        "for cat in categories:\n",
        "    if cat in category2index: continue\n",
        "    category2index[cat] = len(category2index)\n",
        "print(category2index)\n",
        "#{'movie-enter': 0, 'it-life-hack': 1, 'kaden-channel': 2, 'topic-news': 3, 'livedoor-homme': 4, 'peachy': 5, 'sports-watch': 6, 'dokujo-tsushin': 7, 'smax': 8}\n",
        "\n",
        "def category2tensor(cat):\n",
        "    return torch.tensor([category2index[cat]], dtype=torch.long)\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'H=0.1': 0, 'H=0.9': 1, 'H=0.7': 2, 'H=0.8': 3, 'H=0.6': 4, 'H=0.5': 5, 'H=0.4': 6, 'H=0.3': 7, 'H=0.2': 8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmcMN4RA9XjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e19c95eb-9d27-41a7-f917-4a24057e388a"
      },
      "source": [
        "# print(train_data.index)\n",
        "\n",
        "# ネットワークのロード\n",
        "# CPUとGPUのどちらを使うかを指定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "net = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, TAG_SIZE).to(device)\n",
        "# デバイスの確認\n",
        "print(\"Device: {}\".format(device))\n",
        "\n",
        "# 入力次元数\n",
        "EMBEDDING_DIM = 1\n",
        "# EMBEDDING_DIM = 10\n",
        "# 隠れ層の次元数\n",
        "HIDDEN_DIM = 128\n",
        "# データ全体の単語数\n",
        "# VOCAB_SIZE = len(word2index)\n",
        "# 分類先のカテゴリの数\n",
        "TAG_SIZE = len(categories)\n",
        "# モデル宣言\n",
        "# model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, TAG_SIZE)\n",
        "# 損失関数はNLLLoss()を使う。LogSoftmaxを使う時はこれを使うらしい。\n",
        "loss_function = nn.NLLLoss()\n",
        "# 最適化関数の定義\n",
        "optimizer = optim.Adam(net.parameters())\n",
        "# 最適化の手法はSGDで。lossの減りに時間かかるけど、一旦はこれを使う。\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 損失を保存するリストを作成\n",
        "train_loss_list = []  # 学習損失\n",
        "test_loss_list = []  # 評価損失\n",
        "\n",
        "epoch = 20\n",
        "\n",
        "# 各エポックの合計loss値を格納する\n",
        "losses = []\n",
        "# 100ループ回してみる。（バッチ化とかGPU使ってないので結構時間かかる...）\n",
        "for i in range(epoch):\n",
        "  all_loss = 0\n",
        "  # エポックの進行状況を表示\n",
        "  print('-------------------------------------')\n",
        "  print(\"Epoch: {}/{}\".format(i+1,epoch))\n",
        "\n",
        "  # 損失の初期化\n",
        "  # train_loss = 0  # 学習損失\n",
        "  # test_loss = 0  # 評価損失\n",
        "\n",
        "  # ---学習パート--- #\n",
        "  # ニューラルネットワークを学習モードに設定\n",
        "  # net.train()\n",
        "  for ii in range(0,len(train_data)):\n",
        "        data = train_data.iloc[ii][:-1]\n",
        "        # cat = train_data.iloc[ii][0]\n",
        "        cat = train_data.iloc[ii]['index']\n",
        "        # print(data)\n",
        "        # print(cat)\n",
        "        # モデルが持ってる勾配の情報をリセット\n",
        "        # model.zero_grad()\n",
        "        net.zero_grad()\n",
        "        # 文章を単語IDの系列に変換（modelに食わせられる形に変換）\n",
        "        # inputs = torch.Tensor(data.values.astype(np.float32))\n",
        "        inputs = torch.tensor(np.array(data.astype('f')))\n",
        "        # GPUにTensorを転送\n",
        "        inputs = inputs.to(device)\n",
        "        # 順伝播の結果を受け取る\n",
        "        # out = model(inputs)\n",
        "        out = net(inputs)\n",
        "        # 正解カテゴリをテンソル化\n",
        "        answer = category2tensor(cat)\n",
        "        # GPUにTensorを転送\n",
        "        answer = answer.to(device)\n",
        "        # 正解とのlossを計算\n",
        "        loss = loss_function(out, answer)\n",
        "        # 勾配をセット\n",
        "        loss.backward()\n",
        "        # 逆伝播でパラメータ更新\n",
        "        optimizer.step()\n",
        "        # lossを集計\n",
        "        all_loss += loss.item()\n",
        "  losses.append(all_loss)\n",
        "  print(\"epoch\", i+1, \"\\t\" , \"loss\", all_loss)\n",
        "print(\"done.\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "-------------------------------------\n",
            "Epoch: 1/20\n",
            "epoch 0 \t loss 443.75940442085266\n",
            "-------------------------------------\n",
            "Epoch: 2/20\n",
            "epoch 1 \t loss 441.53929245471954\n",
            "-------------------------------------\n",
            "Epoch: 3/20\n",
            "epoch 2 \t loss 434.569974899292\n",
            "-------------------------------------\n",
            "Epoch: 4/20\n",
            "epoch 3 \t loss 416.3819628357887\n",
            "-------------------------------------\n",
            "Epoch: 5/20\n",
            "epoch 4 \t loss 412.8994579911232\n",
            "-------------------------------------\n",
            "Epoch: 6/20\n",
            "epoch 5 \t loss 450.2494423389435\n",
            "-------------------------------------\n",
            "Epoch: 7/20\n",
            "epoch 6 \t loss 451.4363100528717\n",
            "-------------------------------------\n",
            "Epoch: 8/20\n",
            "epoch 7 \t loss 449.13227915763855\n",
            "-------------------------------------\n",
            "Epoch: 9/20\n",
            "epoch 8 \t loss 447.65450286865234\n",
            "-------------------------------------\n",
            "Epoch: 10/20\n",
            "epoch 9 \t loss 446.8943039178848\n",
            "-------------------------------------\n",
            "Epoch: 11/20\n",
            "epoch 10 \t loss 446.37754344940186\n",
            "-------------------------------------\n",
            "Epoch: 12/20\n",
            "epoch 11 \t loss 445.9653924703598\n",
            "-------------------------------------\n",
            "Epoch: 13/20\n",
            "epoch 12 \t loss 445.552485704422\n",
            "-------------------------------------\n",
            "Epoch: 14/20\n",
            "epoch 13 \t loss 444.98669385910034\n",
            "-------------------------------------\n",
            "Epoch: 15/20\n",
            "epoch 14 \t loss 443.87195563316345\n",
            "-------------------------------------\n",
            "Epoch: 16/20\n",
            "epoch 15 \t loss 441.52518486976624\n",
            "-------------------------------------\n",
            "Epoch: 17/20\n",
            "epoch 16 \t loss 440.1106472015381\n",
            "-------------------------------------\n",
            "Epoch: 18/20\n",
            "epoch 17 \t loss 446.060506939888\n",
            "-------------------------------------\n",
            "Epoch: 19/20\n",
            "epoch 18 \t loss 446.8841234445572\n",
            "-------------------------------------\n",
            "Epoch: 20/20\n",
            "epoch 19 \t loss 445.2168884277344\n",
            "done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "W37NkMH49FFa",
        "outputId": "2df1741a-71e4-404c-b77d-b981b58c1684"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# 損失\n",
        "fig2 = plt.figure()\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(range(1, epoch+1), losses)\n",
        "plt.show()\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
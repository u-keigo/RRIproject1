{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_hurst3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOKeCu5Y8H/QaIb4G5VBtP9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/u-keigo/RRIproject1/blob/main/LSTM_hurst3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "off0ybXVLahB"
      },
      "source": [
        "# ハースト指数を予測する\n",
        "## （バッチ化に挑戦）\n",
        "### 失敗した"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvrAI48HLZs_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e49336b0-3d19-4d20-e4c4-412a05c9a5fe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1mjucopLnUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67a6aaef-586f-412f-fe5e-a2a062fd3b65"
      },
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import linecache\n",
        "\n",
        "\n",
        "def read_Gauss(name):\n",
        "  nums = []  # 整数を入れるリスト\n",
        "  with open(name, 'r', encoding='utf-8') as fin:  # ファイルを開く\n",
        "    for line in fin.readlines():  # 行を読み込んでfor文で回す\n",
        "        try:\n",
        "            line = line.replace('\\n','')\n",
        "            num = float(line)  # 行を整数（int）に変換する\n",
        "        except ValueError as e:\n",
        "            print(e, file=sys.stderr)  # エラーが出たら画面に出力\n",
        "            continue\n",
        "\n",
        "        nums.append(num)  # 変換した整数をリストに保存する\n",
        "  return (nums)\n",
        "\n",
        "\n",
        "# カテゴリを配列で取得\n",
        "drive_dir = \"/content/drive/My Drive/python/\"\n",
        "\n",
        "categories = [name for name in os.listdir(drive_dir + 'data_gauss') if os.path.isdir(drive_dir + \"data_gauss/\" +name)]\n",
        "print(categories)\n",
        "\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['H=0.1', 'H=0.9', 'H=0.7', 'H=0.8', 'H=0.6', 'H=0.5', 'H=0.4', 'H=0.3', 'H=0.2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "AD5YAHdlctKv",
        "outputId": "6d0eab36-db55-4644-9cb4-30896106eadc"
      },
      "source": [
        "import numpy as np\n",
        "DAT = pd.DataFrame(np.zeros(shape=(1024, (32*9))))\n",
        "i=0\n",
        "for cat in categories:\n",
        "    path = drive_dir + \"data_gauss/\" + cat + \"/*.rri\"\n",
        "    files = glob(path)\n",
        "    for text_name in files:\n",
        "      data = pd.Series(read_Gauss(text_name))\n",
        "      DAT.iloc[:, i] = data\n",
        "      DAT.rename(columns={i: cat}, inplace=True)\n",
        "      i = i+1\n",
        "\n",
        "\n",
        "# データフレームシャッフル\n",
        "DAT = DAT.sample(frac=1,axis=1).reset_index(drop=True)\n",
        "DAT.T.head()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>984</th>\n",
              "      <th>985</th>\n",
              "      <th>986</th>\n",
              "      <th>987</th>\n",
              "      <th>988</th>\n",
              "      <th>989</th>\n",
              "      <th>990</th>\n",
              "      <th>991</th>\n",
              "      <th>992</th>\n",
              "      <th>993</th>\n",
              "      <th>994</th>\n",
              "      <th>995</th>\n",
              "      <th>996</th>\n",
              "      <th>997</th>\n",
              "      <th>998</th>\n",
              "      <th>999</th>\n",
              "      <th>1000</th>\n",
              "      <th>1001</th>\n",
              "      <th>1002</th>\n",
              "      <th>1003</th>\n",
              "      <th>1004</th>\n",
              "      <th>1005</th>\n",
              "      <th>1006</th>\n",
              "      <th>1007</th>\n",
              "      <th>1008</th>\n",
              "      <th>1009</th>\n",
              "      <th>1010</th>\n",
              "      <th>1011</th>\n",
              "      <th>1012</th>\n",
              "      <th>1013</th>\n",
              "      <th>1014</th>\n",
              "      <th>1015</th>\n",
              "      <th>1016</th>\n",
              "      <th>1017</th>\n",
              "      <th>1018</th>\n",
              "      <th>1019</th>\n",
              "      <th>1020</th>\n",
              "      <th>1021</th>\n",
              "      <th>1022</th>\n",
              "      <th>1023</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>H=0.7</th>\n",
              "      <td>-0.588034</td>\n",
              "      <td>-0.287743</td>\n",
              "      <td>-0.612864</td>\n",
              "      <td>-0.619655</td>\n",
              "      <td>0.627601</td>\n",
              "      <td>0.123289</td>\n",
              "      <td>1.429318</td>\n",
              "      <td>-1.228773</td>\n",
              "      <td>0.106156</td>\n",
              "      <td>0.832035</td>\n",
              "      <td>0.841141</td>\n",
              "      <td>0.213237</td>\n",
              "      <td>1.700141</td>\n",
              "      <td>-0.897730</td>\n",
              "      <td>-0.311464</td>\n",
              "      <td>0.086907</td>\n",
              "      <td>0.544251</td>\n",
              "      <td>0.221683</td>\n",
              "      <td>-0.386928</td>\n",
              "      <td>-1.280346</td>\n",
              "      <td>0.557177</td>\n",
              "      <td>-0.049411</td>\n",
              "      <td>0.172290</td>\n",
              "      <td>-0.003512</td>\n",
              "      <td>0.115060</td>\n",
              "      <td>-0.402086</td>\n",
              "      <td>-0.112538</td>\n",
              "      <td>0.279109</td>\n",
              "      <td>-0.227215</td>\n",
              "      <td>1.310820</td>\n",
              "      <td>1.380144</td>\n",
              "      <td>1.739183</td>\n",
              "      <td>0.865346</td>\n",
              "      <td>0.921820</td>\n",
              "      <td>-0.489017</td>\n",
              "      <td>0.940501</td>\n",
              "      <td>-0.156376</td>\n",
              "      <td>0.229370</td>\n",
              "      <td>-0.299739</td>\n",
              "      <td>-0.554748</td>\n",
              "      <td>...</td>\n",
              "      <td>0.828110</td>\n",
              "      <td>0.576683</td>\n",
              "      <td>0.091132</td>\n",
              "      <td>-1.015567</td>\n",
              "      <td>-1.930997</td>\n",
              "      <td>0.272719</td>\n",
              "      <td>-0.656398</td>\n",
              "      <td>0.228674</td>\n",
              "      <td>0.715173</td>\n",
              "      <td>0.512658</td>\n",
              "      <td>-0.169764</td>\n",
              "      <td>-0.611219</td>\n",
              "      <td>-1.819088</td>\n",
              "      <td>0.596954</td>\n",
              "      <td>-0.042182</td>\n",
              "      <td>0.500645</td>\n",
              "      <td>-0.862991</td>\n",
              "      <td>-1.357412</td>\n",
              "      <td>-0.604545</td>\n",
              "      <td>-0.312486</td>\n",
              "      <td>-0.617157</td>\n",
              "      <td>0.519595</td>\n",
              "      <td>0.988267</td>\n",
              "      <td>1.350393</td>\n",
              "      <td>0.908976</td>\n",
              "      <td>0.893551</td>\n",
              "      <td>1.088291</td>\n",
              "      <td>-1.407421</td>\n",
              "      <td>-0.386190</td>\n",
              "      <td>-0.254563</td>\n",
              "      <td>-0.566499</td>\n",
              "      <td>-0.544298</td>\n",
              "      <td>-0.065700</td>\n",
              "      <td>0.478115</td>\n",
              "      <td>-1.220781</td>\n",
              "      <td>-0.668833</td>\n",
              "      <td>-0.604168</td>\n",
              "      <td>-0.212697</td>\n",
              "      <td>-1.914012</td>\n",
              "      <td>-0.249812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H=0.2</th>\n",
              "      <td>0.845461</td>\n",
              "      <td>-2.134457</td>\n",
              "      <td>0.603385</td>\n",
              "      <td>-0.495729</td>\n",
              "      <td>-0.704056</td>\n",
              "      <td>0.918272</td>\n",
              "      <td>0.517226</td>\n",
              "      <td>-1.776804</td>\n",
              "      <td>0.545644</td>\n",
              "      <td>-0.060963</td>\n",
              "      <td>1.278737</td>\n",
              "      <td>-1.287791</td>\n",
              "      <td>0.537247</td>\n",
              "      <td>0.304055</td>\n",
              "      <td>-0.146142</td>\n",
              "      <td>0.447847</td>\n",
              "      <td>0.363029</td>\n",
              "      <td>0.660639</td>\n",
              "      <td>-1.270708</td>\n",
              "      <td>0.371136</td>\n",
              "      <td>-0.258741</td>\n",
              "      <td>0.359309</td>\n",
              "      <td>-0.546128</td>\n",
              "      <td>0.655015</td>\n",
              "      <td>-0.376205</td>\n",
              "      <td>-0.697187</td>\n",
              "      <td>0.086764</td>\n",
              "      <td>0.252157</td>\n",
              "      <td>0.388421</td>\n",
              "      <td>-1.334445</td>\n",
              "      <td>0.987701</td>\n",
              "      <td>0.020077</td>\n",
              "      <td>0.837301</td>\n",
              "      <td>-1.128812</td>\n",
              "      <td>0.734057</td>\n",
              "      <td>-1.249393</td>\n",
              "      <td>0.741410</td>\n",
              "      <td>-1.014634</td>\n",
              "      <td>-0.561648</td>\n",
              "      <td>-0.569003</td>\n",
              "      <td>...</td>\n",
              "      <td>0.687570</td>\n",
              "      <td>-0.947287</td>\n",
              "      <td>1.216429</td>\n",
              "      <td>-0.581162</td>\n",
              "      <td>0.865358</td>\n",
              "      <td>0.915445</td>\n",
              "      <td>-0.775494</td>\n",
              "      <td>-0.248142</td>\n",
              "      <td>0.115697</td>\n",
              "      <td>0.874609</td>\n",
              "      <td>-2.019860</td>\n",
              "      <td>1.240208</td>\n",
              "      <td>-1.079387</td>\n",
              "      <td>0.454000</td>\n",
              "      <td>1.633866</td>\n",
              "      <td>0.374936</td>\n",
              "      <td>-1.351849</td>\n",
              "      <td>-0.233934</td>\n",
              "      <td>2.166926</td>\n",
              "      <td>-1.195369</td>\n",
              "      <td>-0.663495</td>\n",
              "      <td>-0.131335</td>\n",
              "      <td>-0.254910</td>\n",
              "      <td>-0.360757</td>\n",
              "      <td>1.710536</td>\n",
              "      <td>-0.971714</td>\n",
              "      <td>-0.179198</td>\n",
              "      <td>1.626823</td>\n",
              "      <td>-0.360994</td>\n",
              "      <td>-0.288578</td>\n",
              "      <td>0.391594</td>\n",
              "      <td>-0.522012</td>\n",
              "      <td>-0.132008</td>\n",
              "      <td>-0.263979</td>\n",
              "      <td>2.017746</td>\n",
              "      <td>-0.202105</td>\n",
              "      <td>-1.697152</td>\n",
              "      <td>0.688652</td>\n",
              "      <td>-1.242811</td>\n",
              "      <td>0.387248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H=0.1</th>\n",
              "      <td>0.735359</td>\n",
              "      <td>-0.458145</td>\n",
              "      <td>0.605607</td>\n",
              "      <td>1.097721</td>\n",
              "      <td>1.174084</td>\n",
              "      <td>-1.448871</td>\n",
              "      <td>-2.210869</td>\n",
              "      <td>2.676773</td>\n",
              "      <td>-0.878935</td>\n",
              "      <td>-0.787031</td>\n",
              "      <td>0.117229</td>\n",
              "      <td>0.242599</td>\n",
              "      <td>-0.557570</td>\n",
              "      <td>2.313303</td>\n",
              "      <td>0.010366</td>\n",
              "      <td>-2.190669</td>\n",
              "      <td>0.109482</td>\n",
              "      <td>-0.707026</td>\n",
              "      <td>1.479448</td>\n",
              "      <td>-0.933438</td>\n",
              "      <td>0.252160</td>\n",
              "      <td>-0.743883</td>\n",
              "      <td>-0.677097</td>\n",
              "      <td>1.258819</td>\n",
              "      <td>-0.477889</td>\n",
              "      <td>-1.904090</td>\n",
              "      <td>3.422584</td>\n",
              "      <td>-1.030297</td>\n",
              "      <td>0.292679</td>\n",
              "      <td>0.630351</td>\n",
              "      <td>-1.419021</td>\n",
              "      <td>0.293425</td>\n",
              "      <td>-0.090718</td>\n",
              "      <td>-0.000987</td>\n",
              "      <td>0.785013</td>\n",
              "      <td>0.890676</td>\n",
              "      <td>-0.359453</td>\n",
              "      <td>-1.414382</td>\n",
              "      <td>1.125361</td>\n",
              "      <td>0.311273</td>\n",
              "      <td>...</td>\n",
              "      <td>0.408133</td>\n",
              "      <td>1.648702</td>\n",
              "      <td>-0.261273</td>\n",
              "      <td>-0.438295</td>\n",
              "      <td>-0.703796</td>\n",
              "      <td>1.099677</td>\n",
              "      <td>-0.549911</td>\n",
              "      <td>-0.402123</td>\n",
              "      <td>0.514996</td>\n",
              "      <td>0.536853</td>\n",
              "      <td>-1.001161</td>\n",
              "      <td>-0.015065</td>\n",
              "      <td>0.539008</td>\n",
              "      <td>-0.024225</td>\n",
              "      <td>1.734411</td>\n",
              "      <td>-0.537129</td>\n",
              "      <td>-0.510876</td>\n",
              "      <td>0.059888</td>\n",
              "      <td>1.084015</td>\n",
              "      <td>-1.488814</td>\n",
              "      <td>0.126125</td>\n",
              "      <td>0.397503</td>\n",
              "      <td>-0.938776</td>\n",
              "      <td>0.106337</td>\n",
              "      <td>1.364418</td>\n",
              "      <td>0.240339</td>\n",
              "      <td>-1.908295</td>\n",
              "      <td>0.149401</td>\n",
              "      <td>0.478579</td>\n",
              "      <td>0.765899</td>\n",
              "      <td>0.103079</td>\n",
              "      <td>1.470723</td>\n",
              "      <td>-0.775111</td>\n",
              "      <td>-0.708011</td>\n",
              "      <td>0.647333</td>\n",
              "      <td>0.060977</td>\n",
              "      <td>0.775501</td>\n",
              "      <td>-1.280055</td>\n",
              "      <td>1.471662</td>\n",
              "      <td>-0.244385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H=0.8</th>\n",
              "      <td>-0.006870</td>\n",
              "      <td>-1.001835</td>\n",
              "      <td>-2.078722</td>\n",
              "      <td>-0.323067</td>\n",
              "      <td>-1.355498</td>\n",
              "      <td>-1.589519</td>\n",
              "      <td>-0.611305</td>\n",
              "      <td>-0.073338</td>\n",
              "      <td>0.529313</td>\n",
              "      <td>0.848954</td>\n",
              "      <td>-0.900225</td>\n",
              "      <td>-0.515004</td>\n",
              "      <td>-0.048927</td>\n",
              "      <td>-0.362034</td>\n",
              "      <td>-1.352163</td>\n",
              "      <td>-0.974065</td>\n",
              "      <td>-0.822506</td>\n",
              "      <td>0.755624</td>\n",
              "      <td>0.361086</td>\n",
              "      <td>0.879381</td>\n",
              "      <td>0.823688</td>\n",
              "      <td>0.589071</td>\n",
              "      <td>0.940721</td>\n",
              "      <td>0.099965</td>\n",
              "      <td>-0.379111</td>\n",
              "      <td>-0.699423</td>\n",
              "      <td>-0.827383</td>\n",
              "      <td>-0.002784</td>\n",
              "      <td>0.056848</td>\n",
              "      <td>0.091292</td>\n",
              "      <td>-0.463169</td>\n",
              "      <td>-0.001980</td>\n",
              "      <td>0.193070</td>\n",
              "      <td>-0.600625</td>\n",
              "      <td>-0.759616</td>\n",
              "      <td>-0.205024</td>\n",
              "      <td>1.299127</td>\n",
              "      <td>1.245043</td>\n",
              "      <td>-1.170399</td>\n",
              "      <td>-1.169787</td>\n",
              "      <td>...</td>\n",
              "      <td>1.360221</td>\n",
              "      <td>0.526232</td>\n",
              "      <td>-0.183582</td>\n",
              "      <td>-0.098885</td>\n",
              "      <td>1.157669</td>\n",
              "      <td>-0.803244</td>\n",
              "      <td>0.433923</td>\n",
              "      <td>-0.071947</td>\n",
              "      <td>-0.147765</td>\n",
              "      <td>-0.257753</td>\n",
              "      <td>0.589102</td>\n",
              "      <td>0.856362</td>\n",
              "      <td>0.716965</td>\n",
              "      <td>1.306454</td>\n",
              "      <td>-0.176231</td>\n",
              "      <td>-1.031076</td>\n",
              "      <td>0.554025</td>\n",
              "      <td>0.010201</td>\n",
              "      <td>-0.317939</td>\n",
              "      <td>-0.126475</td>\n",
              "      <td>-1.017969</td>\n",
              "      <td>1.299803</td>\n",
              "      <td>1.209251</td>\n",
              "      <td>-0.988085</td>\n",
              "      <td>0.619772</td>\n",
              "      <td>-0.242597</td>\n",
              "      <td>-1.353245</td>\n",
              "      <td>-1.638247</td>\n",
              "      <td>-1.483129</td>\n",
              "      <td>-0.933041</td>\n",
              "      <td>-1.978215</td>\n",
              "      <td>-0.491036</td>\n",
              "      <td>0.003471</td>\n",
              "      <td>1.726187</td>\n",
              "      <td>1.293880</td>\n",
              "      <td>2.341683</td>\n",
              "      <td>1.206171</td>\n",
              "      <td>0.940136</td>\n",
              "      <td>0.956258</td>\n",
              "      <td>1.303473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H=0.7</th>\n",
              "      <td>-0.622769</td>\n",
              "      <td>-0.928692</td>\n",
              "      <td>-0.002247</td>\n",
              "      <td>-0.169483</td>\n",
              "      <td>0.326179</td>\n",
              "      <td>0.126091</td>\n",
              "      <td>1.391152</td>\n",
              "      <td>1.860877</td>\n",
              "      <td>-0.545745</td>\n",
              "      <td>-0.629531</td>\n",
              "      <td>-0.584679</td>\n",
              "      <td>0.952096</td>\n",
              "      <td>0.933402</td>\n",
              "      <td>2.316608</td>\n",
              "      <td>0.278979</td>\n",
              "      <td>1.027313</td>\n",
              "      <td>0.104455</td>\n",
              "      <td>0.226755</td>\n",
              "      <td>0.873526</td>\n",
              "      <td>-0.255695</td>\n",
              "      <td>1.273273</td>\n",
              "      <td>-1.224300</td>\n",
              "      <td>0.872979</td>\n",
              "      <td>0.878399</td>\n",
              "      <td>0.688676</td>\n",
              "      <td>-0.426905</td>\n",
              "      <td>1.798752</td>\n",
              "      <td>0.314131</td>\n",
              "      <td>0.700635</td>\n",
              "      <td>2.093294</td>\n",
              "      <td>1.539120</td>\n",
              "      <td>0.268264</td>\n",
              "      <td>0.260109</td>\n",
              "      <td>-0.091717</td>\n",
              "      <td>-0.948666</td>\n",
              "      <td>0.314080</td>\n",
              "      <td>1.714060</td>\n",
              "      <td>2.298109</td>\n",
              "      <td>-0.336998</td>\n",
              "      <td>0.382671</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.279079</td>\n",
              "      <td>-0.217160</td>\n",
              "      <td>0.940807</td>\n",
              "      <td>-0.060320</td>\n",
              "      <td>0.764533</td>\n",
              "      <td>-1.156753</td>\n",
              "      <td>0.339584</td>\n",
              "      <td>-0.967094</td>\n",
              "      <td>-0.225595</td>\n",
              "      <td>-0.891379</td>\n",
              "      <td>-1.693407</td>\n",
              "      <td>-0.051531</td>\n",
              "      <td>2.099809</td>\n",
              "      <td>-0.658460</td>\n",
              "      <td>0.276479</td>\n",
              "      <td>0.843138</td>\n",
              "      <td>0.655029</td>\n",
              "      <td>-0.054641</td>\n",
              "      <td>0.039340</td>\n",
              "      <td>0.319541</td>\n",
              "      <td>-0.920559</td>\n",
              "      <td>-0.354681</td>\n",
              "      <td>-0.740128</td>\n",
              "      <td>0.201373</td>\n",
              "      <td>1.426554</td>\n",
              "      <td>-0.977202</td>\n",
              "      <td>0.006415</td>\n",
              "      <td>1.263019</td>\n",
              "      <td>1.594291</td>\n",
              "      <td>1.257627</td>\n",
              "      <td>-0.034472</td>\n",
              "      <td>0.905764</td>\n",
              "      <td>-1.071554</td>\n",
              "      <td>0.301455</td>\n",
              "      <td>0.610263</td>\n",
              "      <td>-1.483988</td>\n",
              "      <td>-1.980802</td>\n",
              "      <td>-1.403797</td>\n",
              "      <td>-0.069286</td>\n",
              "      <td>1.016971</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1024 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2     ...      1021      1022      1023\n",
              "H=0.7 -0.588034 -0.287743 -0.612864  ... -0.212697 -1.914012 -0.249812\n",
              "H=0.2  0.845461 -2.134457  0.603385  ...  0.688652 -1.242811  0.387248\n",
              "H=0.1  0.735359 -0.458145  0.605607  ... -1.280055  1.471662 -0.244385\n",
              "H=0.8 -0.006870 -1.001835 -2.078722  ...  0.940136  0.956258  1.303473\n",
              "H=0.7 -0.622769 -0.928692 -0.002247  ... -1.403797 -0.069286  1.016971\n",
              "\n",
              "[5 rows x 1024 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nQbIVvqdDWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "774fdc3c-60fc-4df7-d80d-d88edd77fa6f"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "category2index = {}\n",
        "for cat in categories:\n",
        "    if cat in category2index: continue\n",
        "    category2index[cat] = len(category2index)\n",
        "print(category2index)\n",
        "\n",
        "\n",
        "def category2tensor(cat):\n",
        "    return torch.tensor([category2index[cat]], dtype=torch.long)\n",
        "\n",
        "# データフレームの形状変更・indexの要素化\n",
        "df = DAT.T\n",
        "df['index'] = df.index\n",
        "\n",
        "df_label = df.loc[:, 'index']\n",
        "df_data = df.drop('index', axis=1)\n",
        "\n",
        "dlen = len(DAT.columns)  # 288\n",
        "# train, test = train_test_split(df, test_size=0.3,random_state=0)\n",
        "train_x, test_x, train_y, test_y = train_test_split(df_data, df_label, test_size=0.3, random_state=0)\n",
        "\n",
        "# train_x2 = torch.LongTensor(train_x.values)\n",
        "# test_x2 = torch.LongTensor(test_x.values)\n",
        "train_x2 = torch.FloatTensor(train_x.values)\n",
        "test_x2 = torch.FloatTensor(test_x.values)\n",
        "# train_x2 = torch.cuda.LongTensor(train_x.values)\n",
        "# test_x2 = torch.cuda.LongTensor(test_x.values)\n",
        "train_y2, _ = pd.factorize(train_y)   # カテゴリー変数を数値に変換\n",
        "test_y2, _ = pd.factorize(test_y)\n",
        "\n",
        "train_y2 = torch.FloatTensor(train_y2)\n",
        "test_y2 = torch.FloatTensor(test_y2)\n",
        "\n",
        "# 次元を増やさないといけない？\n",
        "train_x2 = train_x2.unsqueeze(2)\n",
        "train_y2 = train_y2.unsqueeze(1)\n",
        "test_x2 = test_x2.unsqueeze(2)\n",
        "test_y2 = test_y2.unsqueeze(1)\n",
        "\n",
        "\n",
        "# train_x = np.array(train.iloc[:][1:].astype('f'))\n",
        "# train_y = np.array(train.iloc[:].index)\n",
        "# train_y = train.iloc[:].index.to_list()\n",
        "# test_x = np.array(test.iloc[:][:-1].astype('f'))\n",
        "# test_y = np.array(test.iloc[:].index)\n",
        "\n",
        "\n",
        "\n",
        "# train_y2 = category2tensor(train_y)\n",
        "\n",
        "# 特徴量とラベルを結合したデータセットを作成\n",
        "train_dataset = TensorDataset(train_x2, train_y2)\n",
        "test_dataset = TensorDataset(test_x2, test_y2)\n",
        "\n",
        "\n",
        "# # データの形状を確認\n",
        "# print(\"train_data size: {}\". format(train_dataset.shape))\n",
        "# print(\"test_data size: {}\". format(test_dataset.shape))\n",
        "\n",
        "\n",
        "# DataLoaderを使って、データセットを128個のミニパッチに分ける\n",
        "# ミニパッチサイズを指定したデータローダを作成\n",
        "train_batch = DataLoader(\n",
        "    dataset = train_dataset,   # データセットの指定\n",
        "    batch_size = 32,   # バッチサイズの指定\n",
        "    shuffle = True,    # シャッフルするかどうかの指定\n",
        "    num_workers = 2)   # コアの数\n",
        "\n",
        "test_batch = DataLoader(\n",
        "    dataset = test_dataset,   # データセットの指定\n",
        "    batch_size = 32,   # バッチサイズの指定\n",
        "    shuffle = False,    # シャッフルするかどうかの指定\n",
        "    num_workers = 2)   # コアの数\n",
        "\n",
        "# train_batch = train_batch.unsqueeze()\n",
        "\n",
        "# ミニバッチデータセットの確認\n",
        "for data, label in train_batch:\n",
        "  print(\"batch data size: {}\". format(data.size()))  # バッチの入力データサイズ\n",
        "  print(\"batch label size: {}\". format(label.size()))   # バッチのラベルサイズ\n",
        "  break\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'H=0.1': 0, 'H=0.9': 1, 'H=0.7': 2, 'H=0.8': 3, 'H=0.6': 4, 'H=0.5': 5, 'H=0.4': 6, 'H=0.3': 7, 'H=0.2': 8}\n",
            "batch data size: torch.Size([32, 1024, 1])\n",
            "batch label size: torch.Size([32, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPSXmCWA6pc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "374e4ec0-ded8-424c-b983-3f84140c8e42"
      },
      "source": [
        "# nn.Moduleを継承して新しいクラスを作る。決まり文句\n",
        "class LSTMClassifier(nn.Module):\n",
        "    # モデルで使う各ネットワークをコンストラクタで定義\n",
        "    def __init__(self, embedding_dim, hidden_dim, tagset_size):\n",
        "        # 親クラスのコンストラクタ。決まり文句\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # LSTMの隠れ層。これ１つでOK。超便利。\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)   # inputsの最初の次元がバッチサイズとして認識される\n",
        "        # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "        self.softmax = nn.LogSoftmax()\n",
        "\n",
        "    # 順伝播処理はforward関数に記載\n",
        "    def forward(self, embeds):\n",
        "        # 2次元テンソルをLSTMに食わせられる様にviewで３次元テンソルにした上でLSTMへ流す。\n",
        "        # 上記で説明した様にmany to oneのタスクを解きたいので、第二戻り値だけ使う。\n",
        "        # embeds.size() = (batch_size × len(sentence) × embedding_dim)\n",
        "        _, lstm_out = self.lstm(embeds)\n",
        "        # lstm_out[0]は３次元テンソルになってしまっているので2次元に調整して全結合。\n",
        "        tag_space = self.hidden2tag(lstm_out[0])\n",
        "        # tag_space.size() = (1 × batch_size × tagset_size)\n",
        "\n",
        "        # softmaxに食わせて、確率として表現\n",
        "        # (batch_size × tagset_size)にするためにsqueeze()する\n",
        "        tag_scores = self.softmax(tag_space.squeeze())\n",
        "        # tag_scores.size() = (batch_size × tagset_size)\n",
        "\n",
        "        return tag_scores\n",
        "\n",
        "category2index = {}\n",
        "for cat in categories:\n",
        "    if cat in category2index: continue\n",
        "    category2index[cat] = len(category2index)\n",
        "print(category2index)\n",
        "\n",
        "def category2tensor(cat):\n",
        "    return torch.tensor([category2index[cat]], dtype=torch.long)\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'H=0.1': 0, 'H=0.9': 1, 'H=0.7': 2, 'H=0.8': 3, 'H=0.6': 4, 'H=0.5': 5, 'H=0.4': 6, 'H=0.3': 7, 'H=0.2': 8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmcMN4RA9XjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41a5f544-581d-4026-c071-7f8c3ee217dc"
      },
      "source": [
        "# 入力次元数\n",
        "EMBEDDING_DIM = 1\n",
        "# 隠れ層の次元数\n",
        "HIDDEN_DIM = 128\n",
        "# 分類先のカテゴリの数\n",
        "TAG_SIZE = len(categories)\n",
        "\n",
        "# ネットワークのロード\n",
        "# CPUとGPUのどちらを使うかを指定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "net = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, TAG_SIZE).to(device)\n",
        "# デバイスの確認\n",
        "print(\"Device: {}\".format(device))\n",
        "\n",
        "\n",
        "# 損失関数はNLLLoss()を使う。LogSoftmaxを使う時はこれを使うらしい。\n",
        "# loss_function = nn.NLLLoss()\n",
        "loss_function = nn.MSELoss()  # 損失関数（平均二乗誤差：MSE）\n",
        "# 最適化関数の定義\n",
        "optimizer = optim.Adam(net.parameters())\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# 損失を保存するリストを作成\n",
        "train_loss_list = []  # 学習損失\n",
        "test_loss_list = []  # 評価損失\n",
        "\n",
        "epoch = 100\n",
        "\n",
        "for i in range(epoch):\n",
        "  all_loss = 0\n",
        "  # エポックの進行状況を表示\n",
        "  print('-------------------------------------')\n",
        "  print(\"Epoch: {}/{}\".format(i+1,epoch))\n",
        "\n",
        "  # 損失の初期化\n",
        "  train_loss = 0  # 学習損失\n",
        "  test_loss = 0  # 評価損失\n",
        "\n",
        "  # ---学習パート--- #\n",
        "  # ニューラルネットワークを学習モードに設定\n",
        "  net.train()\n",
        "  # for ii in range(0,len(train_batch)):\n",
        "  for j, (xx, yy) in enumerate(train_batch):\n",
        "    # for data, cat in enumerate(train_batch[ii]):\n",
        "        # data = train_batch.dataset\n",
        "        # cat = train_batch[ii]\n",
        "        # モデルが持ってる勾配の情報をリセット\n",
        "        net.zero_grad()\n",
        "        # tensor型に変換\n",
        "        # inputs = torch.tensor(np.array(data.astype('f')))\n",
        "        # inputs = torch.cuda.LongTensor(xx)\n",
        "        inputs = xx.cuda()\n",
        "        # GPUにTensorを転送\n",
        "        inputs = inputs.to(device)\n",
        "        # inputs2 = torch.cuda.LongTensor(inputs)\n",
        "        # 順伝播の結果を受け取る\n",
        "        out = net(inputs)\n",
        "        # 正解カテゴリをテンソル化\n",
        "        # answer = category2tensor(cat)\n",
        "        # answer = category2tensor(yy.cuda())\n",
        "        answer = yy.cuda()\n",
        "        # GPUにTensorを転送\n",
        "        answer = answer.to(device)\n",
        "        # 正解とのlossを計算\n",
        "        loss = loss_function(out, answer)\n",
        "        # 勾配をセット\n",
        "        loss.backward()\n",
        "        # 逆伝播でパラメータ更新\n",
        "        optimizer.step()\n",
        "        # lossを集計\n",
        "        train_loss += loss.item()\n",
        "  \n",
        "  # ミニバッチの平均の損失を計算\n",
        "  batch_train_loss = train_loss / len(train_batch)\n",
        "  # ---学習パートはここまで--- #\n",
        "\n",
        "  # ---評価パート--- #\n",
        "  # ニューラルネットワークを評価モードに設定\n",
        "  net.eval()\n",
        "  # 評価時の計算で自動微分機能をオフにする\n",
        "  with torch.no_grad():\n",
        "    for j, (xx, yy) in enumerate(test_batch):\n",
        "  # for data, cat in enumerate(train_batch[ii]):\n",
        "        # data = train_batch.dataset\n",
        "        # cat = train_batch[ii]\n",
        "        # モデルが持ってる勾配の情報をリセット\n",
        "        # net.zero_grad()\n",
        "        # tensor型に変換\n",
        "        # inputs = torch.tensor(np.array(data.astype('f')))\n",
        "        # inputs = torch.cuda.LongTensor(xx)\n",
        "        inputs = xx.cuda()\n",
        "        # GPUにTensorを転送\n",
        "        inputs = inputs.to(device)\n",
        "        # inputs2 = torch.cuda.LongTensor(inputs)\n",
        "        # 順伝播の結果を受け取る\n",
        "        out = net(inputs)\n",
        "        # 正解カテゴリをテンソル化\n",
        "        # answer = category2tensor(cat)\n",
        "        # answer = category2tensor(yy.cuda())\n",
        "        answer = yy.cuda()\n",
        "        # GPUにTensorを転送\n",
        "        answer = answer.to(device)\n",
        "        # 正解とのlossを計算\n",
        "        loss = loss_function(out, answer)\n",
        "        # 勾配をセット\n",
        "        # loss.backward()\n",
        "        # 逆伝播でパラメータ更新\n",
        "        # optimizer.step()\n",
        "        # lossを集計\n",
        "        test_loss += loss.item()\n",
        "\n",
        "\n",
        "  # # ---学習パート--- #\n",
        "  # # ニューラルネットワークを学習モードに設定\n",
        "  # net.train()\n",
        "  # # ミニバッチごとにデータをロードし学習\n",
        "  # for data, label in train_batch:\n",
        "  #   # GPUにTensorを転送\n",
        "  #   data = data.to(device)\n",
        "  #   label = label.to(device)\n",
        "\n",
        "  #   # 勾配を初期化\n",
        "  #   optimizer.zero_grad()\n",
        "  #   # データを入力して予測値を計算（順伝搬）\n",
        "  #   y_pred = net(data)\n",
        "  #   # 損失（誤差）を計算\n",
        "  #   loss = criterion(y_pred, label)\n",
        "  #   # 勾配の計算（逆伝搬）\n",
        "  #   loss.backward()\n",
        "  #   # パラメータ（重み）の更新\n",
        "  #   optimizer.step()\n",
        "  #   # ミニバッチごとの損失を蓄積\n",
        "  #   train_loss += loss.item()\n",
        "\n",
        "  #   # ミニバッチの平均の損失を計算\n",
        "  # batch_train_loss = train_loss / len(train_batch)\n",
        "  # # ---学習パートはここまで--- #\n",
        "\n",
        "  # # ---評価パート--- #\n",
        "  # # ニューラルネットワークを評価モードに設定\n",
        "  # net.eval()\n",
        "  # # 評価時の計算で自動微分機能をオフにする\n",
        "  # with torch.no_grad():\n",
        "  #   for data, label in test_batch:\n",
        "  #     # GPUにTensorを転送\n",
        "  #     data = data.to(device)\n",
        "  #     label = label.to(device)\n",
        "  #     # データを入力して予測値を計算（順伝搬）\n",
        "  #     y_pred = net(data)\n",
        "  #     # 損失（誤差）を計算\n",
        "  #     loss = criterion(y_pred, label)\n",
        "  #     # ミニバッチごとの損失を蓄積\n",
        "  #     test_loss += loss.item()\n",
        "\n",
        "    \n",
        "  # ミニバッチの平均の損失を計算\n",
        "  batch_test_loss = test_loss / len(test_batch)\n",
        "  # ---評価パートはここまで--- #\n",
        "\n",
        "  # エポックごとに損失を表示\n",
        "  print(\"Train_Loss : {:.2E} Test_Loss: {:.2E}\".format(\n",
        "      batch_train_loss, batch_test_loss))\n",
        "  # 損失をリスト化して保存\n",
        "  train_loss_list.append(batch_train_loss)\n",
        "  test_loss_list.append(batch_test_loss)\n",
        "print(\"done.\")\n",
        "\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "-------------------------------------\n",
            "Epoch: 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9, 9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_Loss : 4.60E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 2/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([23, 1])) that is different to the input size (torch.Size([23, 9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_Loss : 4.30E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 3/100\n",
            "Train_Loss : 4.46E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 4/100\n",
            "Train_Loss : 4.44E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 5/100\n",
            "Train_Loss : 4.34E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 6/100\n",
            "Train_Loss : 4.29E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 7/100\n",
            "Train_Loss : 4.18E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 8/100\n",
            "Train_Loss : 4.47E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 9/100\n",
            "Train_Loss : 4.25E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 10/100\n",
            "Train_Loss : 4.33E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 11/100\n",
            "Train_Loss : 4.43E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 12/100\n",
            "Train_Loss : 4.25E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 13/100\n",
            "Train_Loss : 4.22E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 14/100\n",
            "Train_Loss : 4.24E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 15/100\n",
            "Train_Loss : 4.46E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 16/100\n",
            "Train_Loss : 4.42E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 17/100\n",
            "Train_Loss : 4.29E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 18/100\n",
            "Train_Loss : 4.42E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 19/100\n",
            "Train_Loss : 4.40E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 20/100\n",
            "Train_Loss : 4.37E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 21/100\n",
            "Train_Loss : 4.37E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 22/100\n",
            "Train_Loss : 4.35E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 23/100\n",
            "Train_Loss : 4.48E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 24/100\n",
            "Train_Loss : 4.38E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 25/100\n",
            "Train_Loss : 4.39E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 26/100\n",
            "Train_Loss : 4.49E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 27/100\n",
            "Train_Loss : 4.71E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 28/100\n",
            "Train_Loss : 4.33E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 29/100\n",
            "Train_Loss : 4.23E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 30/100\n",
            "Train_Loss : 4.62E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 31/100\n",
            "Train_Loss : 4.33E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 32/100\n",
            "Train_Loss : 4.51E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 33/100\n",
            "Train_Loss : 4.41E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 34/100\n",
            "Train_Loss : 4.49E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 35/100\n",
            "Train_Loss : 4.29E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 36/100\n",
            "Train_Loss : 4.37E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 37/100\n",
            "Train_Loss : 4.23E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 38/100\n",
            "Train_Loss : 4.50E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 39/100\n",
            "Train_Loss : 4.18E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 40/100\n",
            "Train_Loss : 4.45E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 41/100\n",
            "Train_Loss : 4.39E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 42/100\n",
            "Train_Loss : 4.30E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 43/100\n",
            "Train_Loss : 4.46E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 44/100\n",
            "Train_Loss : 4.34E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 45/100\n",
            "Train_Loss : 4.27E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 46/100\n",
            "Train_Loss : 4.36E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 47/100\n",
            "Train_Loss : 4.34E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 48/100\n",
            "Train_Loss : 4.18E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 49/100\n",
            "Train_Loss : 4.38E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 50/100\n",
            "Train_Loss : 4.41E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 51/100\n",
            "Train_Loss : 4.46E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 52/100\n",
            "Train_Loss : 4.45E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 53/100\n",
            "Train_Loss : 4.34E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 54/100\n",
            "Train_Loss : 4.35E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 55/100\n",
            "Train_Loss : 4.50E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 56/100\n",
            "Train_Loss : 4.45E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 57/100\n",
            "Train_Loss : 4.28E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 58/100\n",
            "Train_Loss : 4.57E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 59/100\n",
            "Train_Loss : 4.29E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 60/100\n",
            "Train_Loss : 4.38E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 61/100\n",
            "Train_Loss : 4.41E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 62/100\n",
            "Train_Loss : 4.37E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 63/100\n",
            "Train_Loss : 4.36E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 64/100\n",
            "Train_Loss : 4.36E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 65/100\n",
            "Train_Loss : 4.37E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 66/100\n",
            "Train_Loss : 4.48E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 67/100\n",
            "Train_Loss : 4.40E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 68/100\n",
            "Train_Loss : 4.28E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 69/100\n",
            "Train_Loss : 4.64E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 70/100\n",
            "Train_Loss : 4.46E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 71/100\n",
            "Train_Loss : 4.27E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 72/100\n",
            "Train_Loss : 4.39E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 73/100\n",
            "Train_Loss : 4.48E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 74/100\n",
            "Train_Loss : 4.30E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 75/100\n",
            "Train_Loss : 4.36E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 76/100\n",
            "Train_Loss : 4.29E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 77/100\n",
            "Train_Loss : 4.43E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 78/100\n",
            "Train_Loss : 4.46E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 79/100\n",
            "Train_Loss : 4.32E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 80/100\n",
            "Train_Loss : 4.34E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 81/100\n",
            "Train_Loss : 4.25E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 82/100\n",
            "Train_Loss : 4.42E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 83/100\n",
            "Train_Loss : 4.07E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 84/100\n",
            "Train_Loss : 4.34E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 85/100\n",
            "Train_Loss : 4.32E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 86/100\n",
            "Train_Loss : 4.48E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 87/100\n",
            "Train_Loss : 4.47E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 88/100\n",
            "Train_Loss : 4.24E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 89/100\n",
            "Train_Loss : 4.21E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 90/100\n",
            "Train_Loss : 4.38E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 91/100\n",
            "Train_Loss : 4.36E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 92/100\n",
            "Train_Loss : 4.42E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 93/100\n",
            "Train_Loss : 4.38E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 94/100\n",
            "Train_Loss : 4.48E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 95/100\n",
            "Train_Loss : 4.30E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 96/100\n",
            "Train_Loss : 4.39E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 97/100\n",
            "Train_Loss : 4.24E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 98/100\n",
            "Train_Loss : 4.50E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 99/100\n",
            "Train_Loss : 4.53E+01 Test_Loss: 4.01E+01\n",
            "-------------------------------------\n",
            "Epoch: 100/100\n",
            "Train_Loss : 4.51E+01 Test_Loss: 4.01E+01\n",
            "done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W37NkMH49FFa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "3575c952-e5ae-4a4c-d711-05b0f78b97f9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# 損失\n",
        "# 損失\n",
        "fig2 = plt.figure()\n",
        "plt.title('Train and Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(range(1, epoch+1), train_loss_list, color='blue',\n",
        "         linestyle='-', label='Train_Loss')\n",
        "plt.plot(range(1, epoch+1), test_loss_list, color='red',\n",
        "         linestyle='--', label='Test_Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}